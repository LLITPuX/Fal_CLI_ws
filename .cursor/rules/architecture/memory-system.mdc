---
description: Cursor Memory System - Comprehensive architecture for temporal knowledge graph with async processing
alwaysApply: false
---
# CURSOR MEMORY SYSTEM ARCHITECTURE

**Version:** 3.0.0  
**Created:** November 13, 2025  
**Status:** ğŸ—ï¸ Design Phase - Ready for Implementation  

---

## Executive Summary

Cursor Memory System - Ñ†Ğµ Ñ–Ğ½Ñ‚ĞµĞ»ĞµĞºÑ‚ÑƒĞ°Ğ»ÑŒĞ½Ğ° ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ¿Ğ°Ğ¼'ÑÑ‚Ñ–, ÑĞºĞ° Ñ‚Ñ€Ğ°Ğ½ÑÑ„Ğ¾Ñ€Ğ¼ÑƒÑ” ĞºĞ¾Ğ¶Ğ½Ñƒ Ğ²Ğ·Ğ°Ñ”Ğ¼Ğ¾Ğ´Ñ–Ñ developerâ†”AI Ğ² ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹ temporal knowledge graph. ĞĞ° Ğ²Ñ–Ğ´Ğ¼Ñ–Ğ½Ñƒ Ğ²Ñ–Ğ´ Ñ‚Ñ€Ğ°Ğ´Ğ¸Ñ†Ñ–Ğ¹Ğ½Ğ¸Ñ… session-based Ğ¿Ñ–Ğ´Ñ…Ğ¾Ğ´Ñ–Ğ², ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ñ€ĞµĞ°Ğ»Ñ–Ğ·ÑƒÑ” ĞºĞ¾Ğ½Ñ†ĞµĞ¿Ñ†Ñ–Ñ **"Ğ¾Ğ´Ğ¸Ğ½ Ñ‡Ğ°Ñ‚ = Ğ¾Ğ´Ğ¸Ğ½ Ğ·Ğ°Ğ¿Ñ€Ğ¾Ñ"** Ğ· Ğ¿Ğ¾Ğ²Ğ½Ñ–ÑÑ‚Ñ ÑĞ²Ñ–Ğ¶Ğ¸Ğ¼ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğ¸Ğ¼ Ğ²Ñ–ĞºĞ½Ğ¾Ğ¼ Ñ‚Ğ° Ğ°ÑĞ¸Ğ½Ñ…Ñ€Ğ¾Ğ½Ğ½Ğ¾Ñ Ğ¾Ğ±Ñ€Ğ¾Ğ±ĞºĞ¾Ñ Ğ´Ğ»Ñ Ğ½ÑƒĞ»ÑŒĞ¾Ğ²Ğ¾Ğ³Ğ¾ overhead.

### Core Principles

1. **Zero-Latency User Experience** - Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ Ğ½Ğ°Ğ´Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ´Ğ¾ Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½Ğ½Ñ Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ—
2. **Temporal Knowledge Graph** - Ğ±ĞµĞ·Ğ¼ĞµĞ¶Ğ½Ğ° Ğ¿Ğ°Ğ¼'ÑÑ‚ÑŒ Ğ±ĞµĞ· session boundaries
3. **Async-First Processing** - background workers Ğ¾Ğ±Ñ€Ğ¾Ğ±Ğ»ÑÑÑ‚ÑŒ Ğ¿Ñ–ÑĞ»Ñ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–
4. **Code-Aware Memory** - Ğ¿Ğ¾Ğ²Ğ½Ğ° Ñ–Ğ½Ñ‚ĞµĞ³Ñ€Ğ°Ñ†Ñ–Ñ Ğ· git Ñ‚Ğ° codebase evolution
5. **Entity-Centric Context** - ÑˆĞ²Ğ¸Ğ´ĞºĞ¸Ğ¹ Ğ¿Ğ¾ÑˆÑƒĞº Ñ‡ĞµÑ€ĞµĞ· pre-extracted entities

---

## Table of Contents

1. [Concept: One Chat = One Query](#concept-one-chat-one-query)
2. [Hot Path: Synchronous Response Pipeline](#hot-path-synchronous-response-pipeline)
3. [Base Knowledge Initialization](#base-knowledge-initialization)
4. [Async Workers Ecosystem](#async-workers-ecosystem)
5. [Enhanced Graph Schema](#enhanced-graph-schema)
6. [Entity Deduplication Strategy](#entity-deduplication-strategy)
7. [Code Evolution Tracking](#code-evolution-tracking)
8. [Response Storage Strategy](#response-storage-strategy)
9. [Performance Targets & SLA](#performance-targets--sla)
10. [Implementation Phase Plan](#implementation-phase-plan)

---

## Concept: One Chat = One Query

### Problem with Traditional Sessions

**Ğ¢Ñ€Ğ°Ğ´Ğ¸Ñ†Ñ–Ğ¹Ğ½Ğ¸Ğ¹ Ğ¿Ñ–Ğ´Ñ…Ñ–Ğ´:**
```
Session 1: [Query1 â†’ Response1 â†’ Query2 â†’ Response2 â†’ ... â†’ Query50]
                        â†‘
                ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ·Ñ€Ğ¾ÑÑ‚Ğ°Ñ”, Ñ‚Ğ¾ĞºĞµĞ½Ğ¸ Ğ²Ğ¸Ñ‚Ñ€Ğ°Ñ‡Ğ°ÑÑ‚ÑŒÑÑ,
                Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ Ğ·Ğ°Ñ…Ğ°Ñ€Ğ°Ñ‰ÑƒÑ”, Ğ²Ğ°Ğ¶ĞºĞ¾ Ğ·Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğµ
```

**ĞĞ°Ñˆ Ğ¿Ñ–Ğ´Ñ…Ñ–Ğ´:**
```
Chat 1: Query1 â†’ [Fresh Context from Graph] â†’ Response1
Chat 2: Query2 â†’ [Fresh Context from Graph] â†’ Response2
Chat 3: Query3 â†’ [Fresh Context from Graph] â†’ Response3
          â†‘
    ĞšĞ¾Ğ¶ĞµĞ½ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ñ€ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚
    Ğ· Ğ±ĞµĞ·Ğ¼ĞµĞ¶Ğ½Ğ¾Ñ— temporal Ğ¿Ğ°Ğ¼'ÑÑ‚Ñ–
```

### Benefits

âœ… **Ğ§Ğ¸ÑÑ‚Ğµ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ½Ğµ Ğ²Ñ–ĞºĞ½Ğ¾** - ĞºĞ¾Ğ¶ĞµĞ½ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ¿Ğ¾Ñ‡Ğ¸Ğ½Ğ°Ñ”Ñ‚ÑŒÑÑ Ğ· Ñ‡Ğ¸ÑÑ‚Ğ¾Ğ³Ğ¾ Ğ°Ñ€ĞºÑƒÑˆĞ°  
âœ… **Ğ ĞµĞ»ĞµĞ²Ğ°Ğ½Ñ‚Ğ½Ğ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚** - Ñ‚Ñ–Ğ»ÑŒĞºĞ¸ Ñ‚Ğµ Ñ‰Ğ¾ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±Ğ½Ğ¾ Ğ·Ğ°Ñ€Ğ°Ğ·  
âœ… **Ğ•ĞºĞ¾Ğ½Ğ¾Ğ¼Ñ–Ñ Ñ‚Ğ¾ĞºĞµĞ½Ñ–Ğ²** - Ğ½Ğµ Ğ¿ĞµÑ€ĞµĞ´Ğ°Ñ”Ğ¼Ğ¾ Ğ²ÑÑ Ñ–ÑÑ‚Ğ¾Ñ€Ñ–Ñ  
âœ… **Temporal awareness** - "ĞŸĞ°Ğ¼'ÑÑ‚Ğ°Ñ”Ñˆ 3 Ğ¼Ñ–ÑÑÑ†Ñ– Ñ‚Ğ¾Ğ¼Ñƒ Ğ¼Ğ¸ Ğ¾Ğ±Ğ³Ğ¾Ğ²Ğ¾Ñ€ÑĞ²Ğ°Ğ»Ğ¸ X?"  
âœ… **Cross-session knowledge** - Ğ·Ğ²'ÑĞ·ĞºĞ¸ Ğ¼Ñ–Ğ¶ Ñ€Ñ–Ğ·Ğ½Ğ¸Ğ¼Ğ¸ "Ñ‡Ğ°Ñ‚Ğ°Ğ¼Ğ¸"  

### How It Works

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Query: "Ğ¯Ğº Ğ¼Ğ¸ Ñ€ĞµĞ°Ğ»Ñ–Ğ·ÑƒĞ²Ğ°Ğ»Ğ¸ Clerk Agent?"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Quick Entity Match        â”‚  â† 10-20ms
    â”‚  "Clerk", "Agent"          â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Graph Context Retrieval   â”‚  â† 30-50ms
    â”‚  - Previous queries about  â”‚
    â”‚    Clerk Agent             â”‚
    â”‚  - Related code files      â”‚
    â”‚  - Architectural decisions â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Generate Response         â”‚  â† 2-3s (Gemini)
    â”‚  with fresh context        â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  Background Processing     â”‚  â† Async (user doesn't wait)
    â”‚  - Vectorize query         â”‚
    â”‚  - Extract entities        â”‚
    â”‚  - Link similar chunks     â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Hot Path: Synchronous Response Pipeline

### Goal: <100ms Overhead Before Response

**Pipeline Steps:**

```python
# 1. Record incoming query (15ms)
query_id = await cursor_repo.create_user_query(
    content=user_query,
    timestamp=datetime.now(),
    status="pending_analysis"  # ĞŸĞ¾Ğ·Ğ½Ğ°Ñ‡ĞºĞ° Ğ´Ğ»Ñ workers
)

# 2. Quick entity match (30ms) - NO VECTORS!
matched_entities = await entity_matcher.find_entities_in_text(
    text=user_query,
    method="substring_match"  # ĞŸÑ€Ğ¾ÑÑ‚Ğ¾ WHERE text CONTAINS entity.name
)

# 3. Link query to entities (10ms)
for entity in matched_entities:
    await cursor_repo.create_mention_edge(query_id, entity.id)
    await cursor_repo.increment_entity_mention_count(entity.id)

# 4. Build context from entities (40ms)
context = await context_builder.build_from_entities(
    query_id=query_id,
    entities=matched_entities,
    include_recent=10,  # ĞÑÑ‚Ğ°Ğ½Ğ½Ñ– 10 messages
    include_related_queries=5,  # Top 5 Ğ¿Ğ¾Ğ²'ÑĞ·Ğ°Ğ½Ğ¸Ñ… Ñ‡ĞµÑ€ĞµĞ· entities
    include_code_context=True  # Ğ¯ĞºÑ‰Ğ¾ entities Ğ¼Ğ°ÑÑ‚ÑŒ code references
)

# 5. Generate response with context (2-3s Gemini)
response = await orchestrator.generate_response(
    query=user_query,
    context=context
)

# 6. Save response (20ms)
response_id = await cursor_repo.create_assistant_response(
    content=response,
    query_id=query_id,
    status="pending_summarization"  # Ğ”Ğ»Ñ summarizer worker
)

# TOTAL OVERHEAD: ~115ms (acceptable!)
# User gets response in ~2.1-3.1s (Gemini time + overhead)
```

### Critical: What We DON'T Do in Hot Path

âŒ **NO OpenAI Embeddings** (200-400ms)  
âŒ **NO Entity Extraction via LLM** (300-500ms)  
âŒ **NO Similarity Search** (Ğ¼Ğ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ Ğ¿Ğ¾Ğ²Ñ–Ğ»ÑŒĞ½Ğ¾ Ğ½Ğ° Ğ²ĞµĞ»Ğ¸ĞºÑ–Ğ¹ Ğ‘Ğ”)  
âŒ **NO Code Parsing** (Ğ¼Ğ¾Ğ¶Ğµ Ğ·Ğ°Ğ¹Ğ¼Ğ°Ñ‚Ğ¸ ÑĞµĞºÑƒĞ½Ğ´Ğ¸)  
âŒ **NO Git Operations** (I/O intensive)  

### Context Building Strategy (Without Vectors)

```cypher
// Cypher query Ğ´Ğ»Ñ ÑˆĞ²Ğ¸Ğ´ĞºĞ¾Ğ³Ğ¾ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñƒ
MATCH (current:UserQuery {id: $query_id})-[:MENTIONS]->(e:Entity)
MATCH (prev:UserQuery)-[:MENTIONS]->(e)
WHERE prev.timestamp < current.timestamp
MATCH (prev)<-[:ANSWERS]-(resp:AssistantResponse)

// ĞĞ¿Ñ†Ñ–Ğ¾Ğ½Ğ°Ğ»ÑŒĞ½Ğ¾: Ğ²ĞºĞ»ÑÑ‡Ğ¸Ñ‚Ğ¸ code context
OPTIONAL MATCH (e)<-[:MENTIONS]-(code:Function)
OPTIONAL MATCH (code)-[:IN_FILE]->(file:CodeFile)

RETURN 
  prev,
  resp,
  e,
  code,
  file
ORDER BY prev.timestamp DESC
LIMIT 10
```

**Execution time:** ~30-50ms (FalkorDB optimized)

---

## Base Knowledge Initialization

### Problem: Cold Start

ĞŸÑ€Ğ¸ Ğ¿ĞµÑ€ÑˆĞ¾Ğ¼Ñƒ Ğ·Ğ°Ğ¿ÑƒÑĞºÑƒ ÑĞ¸ÑÑ‚ĞµĞ¼Ğ° Ğ½Ğµ Ğ¼Ğ°Ñ” entities, Ñ‚Ğ¾Ğ¼Ñƒ Quick Entity Match Ğ½Ñ–Ñ‡Ğ¾Ğ³Ğ¾ Ğ½Ğµ Ğ·Ğ½Ğ°Ğ¹Ğ´Ğµ.

### Solution: Pre-load Knowledge Base

**Ğ©Ğ¾ Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶ÑƒÑ”Ğ¼Ğ¾:**
1. `.cursor/rules/**/*.mdc` - Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ° Ñ‚Ğ° Ğ°Ñ€Ñ…Ñ–Ñ‚ĞµĞºÑ‚ÑƒÑ€Ğ½Ñ– Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸
2. `backend/**/*.py` - Ñ–ÑĞ½ÑƒÑÑ‡Ğ¸Ğ¹ ĞºĞ¾Ğ´ Ğ¿Ñ€Ğ¾ĞµĞºÑ‚Ñƒ
3. `docs/**/*.md` - Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ñ–Ñ
4. `README.md`, `ARCHITECTURE.md` - ĞºĞ»ÑÑ‡Ğ¾Ğ²Ñ– Ğ´Ğ¾ĞºÑƒĞ¼ĞµĞ½Ñ‚Ğ¸

### Initialization Pipeline

```python
async def initialize_knowledge_base():
    """
    Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ base knowledge Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ñ– ÑĞ¸ÑÑ‚ĞµĞ¼Ğ¸.
    
    Ğ’Ğ¸ĞºĞ¾Ğ½ÑƒÑ”Ñ‚ÑŒÑÑ ĞĞ”Ğ˜Ğ Ğ ĞĞ— Ğ°Ğ±Ğ¾ Ğ¿Ñ–ÑĞ»Ñ git pull Ğ· Ğ²ĞµĞ»Ğ¸ĞºĞ¸Ğ¼Ğ¸ Ğ·Ğ¼Ñ–Ğ½Ğ°Ğ¼Ğ¸.
    """
    logger.info("ğŸ§  Initializing Cursor Knowledge Base...")
    
    # 1. Load rules
    await load_rules_documents()
    
    # 2. Parse codebase
    await parse_and_index_codebase()
    
    # 3. Load documentation
    await load_documentation()
    
    # 4. Extract entities from all
    await extract_base_entities()
    
    # 5. Vectorize everything (background)
    await trigger_vectorization_workers()
    
    logger.info("âœ… Knowledge Base initialized")


async def load_rules_documents():
    """Load and chunk .mdc files."""
    rules_files = glob.glob(".cursor/rules/**/*.mdc", recursive=True)
    
    for file_path in rules_files:
        # Read content
        content = Path(file_path).read_text(encoding="utf-8")
        
        # Create Document node
        doc_id = await kb_repo.create_document(
            path=file_path,
            content=content,
            type="rules",
            content_hash=hashlib.sha256(content.encode()).hexdigest()
        )
        
        # Semantic chunking
        chunks = await text_processor.split(content, max_chunk_size=800)
        
        # Save chunks
        for chunk in chunks:
            chunk_id = await kb_repo.create_chunk(
                content=chunk.content,
                document_id=doc_id,
                position=chunk.position,
                chunk_type=chunk.chunk_type,
                status="pending_vectorization"
            )
        
        logger.info(f"ğŸ“„ Loaded: {file_path} ({len(chunks)} chunks)")


async def parse_and_index_codebase():
    """Parse Python codebase into structured nodes."""
    python_files = glob.glob("backend/**/*.py", recursive=True)
    
    for file_path in python_files:
        # Read and parse
        content = Path(file_path).read_text(encoding="utf-8")
        tree = ast.parse(content)
        
        # Create CodeFile node
        file_id = await kb_repo.create_code_file(
            path=file_path,
            language="python",
            lines_count=len(content.splitlines()),
            content_hash=hashlib.sha256(content.encode()).hexdigest(),
            valid_at=datetime.now()
        )
        
        # Extract functions
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                # Get function source
                func_source = ast.get_source_segment(content, node)
                
                # Create Function node
                await kb_repo.create_function(
                    name=node.name,
                    signature=f"def {node.name}(...)",
                    source_code=func_source,
                    start_line=node.lineno,
                    end_line=node.end_lineno,
                    file_id=file_id,
                    status="pending_vectorization"
                )
        
        logger.info(f"ğŸ Parsed: {file_path}")


async def extract_base_entities():
    """
    Extract entities Ğ· ÑƒÑÑŒĞ¾Ğ³Ğ¾ base knowledge.
    
    Ğ’Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑ” OpenAI Ğ² batch mode (Ñ†Ğµ Ğ¾Ğ´Ğ½Ğ¾Ñ€Ğ°Ğ·Ğ¾Ğ²Ğ° Ğ¾Ğ¿ĞµÑ€Ğ°Ñ†Ñ–Ñ).
    """
    # Get all documents and code
    all_content = await kb_repo.get_all_content_for_entity_extraction()
    
    # Batch extraction (groups Ğ¿Ğ¾ 10)
    for batch in chunk_list(all_content, size=10):
        texts = [item.content for item in batch]
        
        # OpenAI batch entity extraction
        entities_batch = await entity_extractor.extract_batch(texts)
        
        # Save entities
        for item, entities in zip(batch, entities_batch):
            for entity in entities:
                # Find or create entity
                entity_id = await kb_repo.find_or_create_entity(
                    name=entity.name,
                    canonical_name=normalize_entity(entity.name, entity.type),
                    type=entity.type,
                    confidence=entity.confidence
                )
                
                # Link entity to source
                await kb_repo.create_mention_edge(
                    source_id=item.id,
                    entity_id=entity_id,
                    context=entity.context
                )
    
    logger.info("ğŸ·ï¸ Base entities extracted")
```

### Knowledge Base Graph Structure

```cypher
(:KnowledgeBase {
  id: "cursor_kb_v1",
  version: "1.0.0",
  initialized_at: "2025-11-13T10:00:00Z",
  total_documents: 50,
  total_chunks: 2500,
  total_entities: 350
})

(:Document {
  id: "uuid",
  path: ".cursor/rules/agents/clerk.mdc",
  type: "rules",
  content: "full text...",
  content_hash: "sha256...",
  loaded_at: "ISO8601",
  chunk_count: 15
})-[:IN_BASE]->(:KnowledgeBase)

(:Chunk {
  id: "uuid",
  content: "chunk text...",
  position: 0,
  chunk_type: "paragraph",
  embedding: [...],  // Ğ”Ğ¾Ğ´Ğ°Ñ”Ñ‚ÑŒÑÑ worker'Ğ¾Ğ¼
  status: "vectorized"
})-[:PART_OF {position: 0}]->(:Document)

(:Entity {
  id: "uuid",
  name: "Clerk Agent",
  canonical_name: "clerk_agent",
  type: "CONCEPT",
  aliases: ["Clerk", "clerk", "ĞŸĞ¸ÑĞ°Ñ€ÑŒ"],
  mention_count: 25,
  first_seen: "ISO8601",
  last_seen: "ISO8601"
})

(:Chunk)-[:MENTIONS {
  position: 123,
  context: "surrounding text"
}]->(:Entity)
```

---

## Async Workers Ecosystem

### Overview: 6 Specialized Workers

| Worker | Priority | Frequency | Purpose |
|--------|----------|-----------|---------|
| Real-time Vectorizer | HIGHEST | 0.1s | Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒÑ” Ğ½Ğ¾Ğ²Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸ Ğ½ĞµĞ³Ğ°Ğ¹Ğ½Ğ¾ |
| Entity Extractor | HIGH | 1s | Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” entities Ğ· Ğ½Ğ¾Ğ²Ğ¸Ñ… queries |
| Similarity Linker | MEDIUM | 2s | Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ” SIMILAR_TO edges |
| Response Summarizer | MEDIUM | 5s | Summarizes Ğ´Ğ¾Ğ²Ğ³Ñ– Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– |
| Entity Deduplicator | LOW | 30s | ĞĞ±'Ñ”Ğ´Ğ½ÑƒÑ” Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ½Ñ– entities |
| Git Change Watcher | LOW | 60s | Ğ’Ñ–Ğ´ÑÑ‚ĞµĞ¶ÑƒÑ” Ğ·Ğ¼Ñ–Ğ½Ğ¸ Ğ² git |

### Worker 1: Real-time Vectorizer

**Goal:** Ğ’ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ”Ğ Ñ‚Ğ¾Ğ³Ğ¾ ÑĞº Ğ¹Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ğ´Ğ¾Ğ±Ğ¸Ñ‚ÑŒÑÑ Ğ´Ğ»Ñ similarity search

```python
# backend/app/agents/cursor/workers/vectorizer.py

import asyncio
from openai import AsyncOpenAI
from app.agents.cursor.repository import CursorRepository

class RealtimeVectorizer:
    """
    ĞĞ°Ğ¹Ğ²Ğ¸Ñ‰Ğ¸Ğ¹ Ğ¿Ñ€Ñ–Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚ - Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒÑ” Ğ½Ğ¾Ğ²Ñ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸ Ğ½ĞµĞ³Ğ°Ğ¹Ğ½Ğ¾.
    """
    
    def __init__(self, repository: CursorRepository, openai_client: AsyncOpenAI):
        self.repo = repository
        self.client = openai_client
        self.model = "text-embedding-3-small"
    
    async def run(self):
        """Ğ‘ĞµĞ·ĞºÑ–Ğ½ĞµÑ‡Ğ½Ğ¸Ğ¹ loop - Ğ¿ĞµÑ€ĞµĞ²Ñ–Ñ€ÑÑ” ĞºĞ¾Ğ¶Ğ½Ñ– 100ms."""
        logger.info("ğŸš€ Real-time Vectorizer started")
        
        while True:
            try:
                # Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ UserQuery.status = "pending_vectorization"
                pending = await self.repo.get_pending_vectorization(limit=1)
                
                if pending:
                    query = pending[0]
                    logger.info(f"âš¡ Vectorizing query {query.id[:8]}...")
                    
                    # Generate embedding
                    response = await self.client.embeddings.create(
                        model=self.model,
                        input=[query.content]
                    )
                    
                    embedding = response.data[0].embedding
                    
                    # Save to DB
                    await self.repo.update_query_embedding(
                        query_id=query.id,
                        embedding=embedding,
                        status="vectorized"
                    )
                    
                    logger.info(f"âœ… Query {query.id[:8]} vectorized")
                
                # Check every 100ms (Ğ´ÑƒĞ¶Ğµ Ñ‡Ğ°ÑÑ‚Ğ¾!)
                await asyncio.sleep(0.1)
                
            except Exception as e:
                logger.error(f"âŒ Vectorizer error: {e}", exc_info=True)
                await asyncio.sleep(1)  # Retry after 1s on error
```

**Why so fast?** Ğ¦ĞµĞ¹ worker Ğ¼Ğ°Ñ” Ğ·Ğ°Ğ±ĞµĞ·Ğ¿ĞµÑ‡Ğ¸Ñ‚Ğ¸ Ñ‰Ğ¾ Ğ´Ğ¾ Ğ¼Ğ¾Ğ¼ĞµĞ½Ñ‚Ñƒ ĞºĞ¾Ğ»Ğ¸ Similarity Linker Ğ¿Ğ¾Ñ‡Ğ½Ğµ Ğ¿Ñ€Ğ°Ñ†ÑĞ²Ğ°Ñ‚Ğ¸, embedding Ğ²Ğ¶Ğµ Ğ³Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¹.

### Worker 2: Entity Extractor

**Goal:** Ğ’Ğ¸Ñ‚ÑĞ³Ñ‚Ğ¸ entities Ğ· Ğ½Ğ¾Ğ²Ğ¸Ñ… queries Ğ²Ğ¸ĞºĞ¾Ñ€Ğ¸ÑÑ‚Ğ¾Ğ²ÑƒÑÑ‡Ğ¸ OpenAI structured output

```python
# backend/app/agents/cursor/workers/entity_extractor.py

class EntityExtractor:
    """
    Ğ’Ğ¸Ñ‚ÑĞ³ÑƒÑ” named entities Ğ· Ğ½Ğ¾Ğ²Ğ¸Ñ… UserQuery Ñ‚Ğ° AssistantResponse.
    """
    
    def __init__(self, repository: CursorRepository, openai_client: AsyncOpenAI):
        self.repo = repository
        self.client = openai_client
        self.model = "gpt-4o-mini"
    
    async def run(self):
        """Check ĞºĞ¾Ğ¶Ğ½Ñƒ ÑĞµĞºÑƒĞ½Ğ´Ñƒ."""
        logger.info("ğŸ·ï¸ Entity Extractor started")
        
        while True:
            try:
                # Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ queries without entities
                pending = await self.repo.get_queries_without_entities(limit=3)
                
                for query in pending:
                    logger.info(f"ğŸ” Extracting entities from {query.id[:8]}...")
                    
                    # Extract entities via OpenAI
                    entities = await self._extract_entities(query.content)
                    
                    logger.info(f"Found {len(entities)} entities")
                    
                    # Process each entity
                    for entity in entities:
                        # Normalize name
                        canonical = normalize_entity(entity.name, entity.type)
                        
                        # Find similar existing entities (fuzzy match)
                        similar = await self._find_similar_entity(
                            canonical,
                            entity.type
                        )
                        
                        if similar and similar.similarity > 0.9:
                            # Link to existing entity
                            entity_id = similar.entity.id
                            logger.info(
                                f"  â†’ Linked to existing: {similar.entity.name}"
                            )
                        else:
                            # Create new entity
                            entity_id = await self.repo.create_entity(
                                name=entity.name,
                                canonical_name=canonical,
                                type=entity.type,
                                confidence=entity.confidence,
                                first_seen=query.timestamp,
                                last_seen=query.timestamp
                            )
                            logger.info(f"  â†’ Created new: {entity.name}")
                        
                        # Create MENTIONS relationship
                        await self.repo.create_mention_edge(
                            source_id=query.id,
                            entity_id=entity_id,
                            position=entity.position,
                            context=entity.context,
                            confidence=entity.confidence
                        )
                    
                    # Mark as processed
                    await self.repo.mark_query_entities_extracted(query.id)
                
                await asyncio.sleep(1)  # Check every second
                
            except Exception as e:
                logger.error(f"âŒ Entity Extractor error: {e}", exc_info=True)
                await asyncio.sleep(5)
    
    async def _extract_entities(self, text: str) -> list[ExtractedEntity]:
        """Extract entities using OpenAI structured output."""
        prompt = f"""Extract all named entities from the following text.

Categories:
- PERSON: People, characters, usernames
- ORG: Organizations, companies, teams
- LOCATION: Countries, cities, places
- TECH: Technologies, programming languages, frameworks, libraries
- CONCEPT: Abstract concepts, methodologies, patterns
- EVENT: Events, conferences, meetings
- COMPONENT: System components, services, modules

Text:
{text}

Return JSON with entities array. Each entity:
- name: exact text from input
- type: category from above
- confidence: 0.0-1.0
- context: 20 chars before and after
- position: character position in text
"""
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.0
        )
        
        result = EntityExtractionResult.model_validate_json(
            response.choices[0].message.content
        )
        
        return result.entities
    
    async def _find_similar_entity(
        self,
        canonical_name: str,
        entity_type: str
    ) -> SimilarEntity | None:
        """
        Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ ÑÑ…Ğ¾Ğ¶Ñƒ entity Ğ² Ğ‘Ğ” (fuzzy matching).
        
        Uses Levenshtein distance or simple substring match.
        """
        cypher = """
        MATCH (e:Entity {type: $type})
        WHERE e.canonical_name = $canonical
           OR $canonical IN e.aliases
           OR e.canonical_name CONTAINS $canonical
           OR $canonical CONTAINS e.canonical_name
        RETURN e, 
               CASE 
                 WHEN e.canonical_name = $canonical THEN 1.0
                 WHEN $canonical IN e.aliases THEN 0.95
                 ELSE 0.8
               END as similarity
        ORDER BY similarity DESC, e.mention_count DESC
        LIMIT 1
        """
        
        result = await self.repo.query(cypher, {
            "canonical": canonical_name,
            "type": entity_type
        })
        
        if result:
            return SimilarEntity(
                entity=Entity(**result[0]["e"]),
                similarity=result[0]["similarity"]
            )
        
        return None
```

### Worker 3: Similarity Linker

**Goal:** Ğ¡Ñ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ¸ SIMILAR_TO edges Ğ¼Ñ–Ğ¶ chunks/queries Ğ¿Ğ¾ embeddings

```python
# backend/app/agents/cursor/workers/similarity_linker.py

class SimilarityLinker:
    """
    Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ” ÑĞµĞ¼Ğ°Ğ½Ñ‚Ğ¸Ñ‡Ğ½Ñ– Ğ·Ğ²'ÑĞ·ĞºĞ¸ Ğ¼Ñ–Ğ¶ nodes Ñ‡ĞµÑ€ĞµĞ· cosine similarity.
    """
    
    def __init__(self, repository: CursorRepository):
        self.repo = repository
        self.threshold = 0.7  # Minimum similarity
    
    async def run(self):
        """Check ĞºĞ¾Ğ¶Ğ½Ñ– 2 ÑĞµĞºÑƒĞ½Ğ´Ğ¸."""
        logger.info("ğŸ”— Similarity Linker started")
        
        while True:
            try:
                # Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ nodes Ğ· embeddings Ğ°Ğ»Ğµ Ğ±ĞµĞ· similarity check
                pending = await self.repo.get_nodes_without_similarity_check(
                    limit=10
                )
                
                for node in pending:
                    logger.info(f"ğŸ” Finding similar nodes for {node.id[:8]}...")
                    
                    # Cosine similarity search
                    similar = await self._find_similar_nodes(
                        embedding=node.embedding,
                        exclude_id=node.id,
                        top_k=10
                    )
                    
                    logger.info(f"Found {len(similar)} similar nodes")
                    
                    # Create edges
                    for sim in similar:
                        await self.repo.create_similarity_edge(
                            from_id=node.id,
                            to_id=sim.node_id,
                            similarity=sim.score,
                            algorithm="cosine"
                        )
                    
                    # Mark as checked
                    await self.repo.mark_similarity_checked(node.id)
                
                await asyncio.sleep(2)  # Every 2 seconds
                
            except Exception as e:
                logger.error(f"âŒ Similarity Linker error: {e}", exc_info=True)
                await asyncio.sleep(5)
    
    async def _find_similar_nodes(
        self,
        embedding: list[float],
        exclude_id: str,
        top_k: int = 10
    ) -> list[SimilarNode]:
        """
        Find similar nodes using cosine similarity.
        
        Strategy:
        1. Get all vectorized nodes from DB
        2. Calculate cosine similarity in-memory (NumPy)
        3. Filter by threshold and return top-K
        
        NOTE: Ğ”Ğ»Ñ >100K nodes Ñ‚Ñ€ĞµĞ±Ğ° Redis Stack VSS Ğ°Ğ±Ğ¾ Qdrant.
        """
        # Get all nodes with embeddings
        all_nodes = await self.repo.get_all_nodes_with_embeddings(
            exclude_id=exclude_id
        )
        
        if not all_nodes:
            return []
        
        # NumPy similarity
        import numpy as np
        from sklearn.metrics.pairwise import cosine_similarity
        
        query_emb = np.array([embedding])
        node_embs = np.array([n.embedding for n in all_nodes])
        
        similarities = cosine_similarity(query_emb, node_embs)[0]
        
        # Filter by threshold
        mask = similarities > self.threshold
        filtered_indices = np.where(mask)[0]
        filtered_sims = similarities[mask]
        
        # Sort and get top-K
        top_indices = filtered_sims.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            node_idx = filtered_indices[idx]
            results.append(SimilarNode(
                node_id=all_nodes[node_idx].id,
                score=float(filtered_sims[idx])
            ))
        
        return results
```

### Worker 4: Response Summarizer

**Goal:** Ğ¡Ñ‚Ğ²Ğ¾Ñ€Ğ¸Ñ‚Ğ¸ summaries Ğ´Ğ»Ñ Ğ´Ğ¾Ğ²Ğ³Ğ¸Ñ… Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ĞµĞ¹ Ñ‚Ğ° Ğ·Ğ±ĞµÑ€ĞµĞ³Ñ‚Ğ¸ Ğ¿Ğ¾Ğ²Ğ½Ğ¸Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ñƒ Ñ„Ğ°Ğ¹Ğ»Ğ°Ñ…

```python
# backend/app/agents/cursor/workers/response_summarizer.py

class ResponseSummarizer:
    """
    Ğ¡Ñ‚Ğ²Ğ¾Ñ€ÑÑ” summaries Ğ´Ğ»Ñ AssistantResponse >1000 chars.
    """
    
    def __init__(
        self,
        repository: CursorRepository,
        openai_client: AsyncOpenAI,
        storage_path: Path
    ):
        self.repo = repository
        self.client = openai_client
        self.storage_path = storage_path
        self.model = "gpt-4o-mini"
    
    async def run(self):
        """Check ĞºĞ¾Ğ¶Ğ½Ñ– 5 ÑĞµĞºÑƒĞ½Ğ´."""
        logger.info("ğŸ“ Response Summarizer started")
        
        while True:
            try:
                # Responses without summary
                pending = await self.repo.get_responses_without_summary(limit=5)
                
                for response in pending:
                    if response.content_length > 1000:
                        logger.info(
                            f"ğŸ“„ Summarizing response {response.id[:8]} "
                            f"({response.content_length} chars)..."
                        )
                        
                        # Save full content to file
                        file_path = await self._save_to_file(response)
                        
                        # Generate summary via OpenAI
                        summary = await self._generate_summary(response.content)
                        
                        # Vectorize summary
                        embedding = await self._vectorize(summary)
                        
                        # Update in graph
                        await self.repo.update_response_summary(
                            response_id=response.id,
                            summary=summary,
                            summary_embedding=embedding,
                            full_content_ref=str(file_path)
                        )
                        
                        logger.info(f"âœ… Summary created ({len(summary)} chars)")
                    else:
                        # Short response - just vectorize as-is
                        embedding = await self._vectorize(response.content)
                        await self.repo.update_response_embedding(
                            response_id=response.id,
                            embedding=embedding
                        )
                
                await asyncio.sleep(5)
                
            except Exception as e:
                logger.error(f"âŒ Summarizer error: {e}", exc_info=True)
                await asyncio.sleep(10)
    
    async def _save_to_file(self, response: AssistantResponse) -> Path:
        """Save full response content to file."""
        # Create directory structure: responses/YYYY/MM/DD/
        date_path = self.storage_path / datetime.now().strftime("%Y/%m/%d")
        date_path.mkdir(parents=True, exist_ok=True)
        
        # File: response_id.md
        file_path = date_path / f"{response.id}.md"
        
        # Write content
        file_path.write_text(response.content, encoding="utf-8")
        
        logger.info(f"ğŸ’¾ Saved to: {file_path}")
        return file_path
    
    async def _generate_summary(self, content: str) -> str:
        """Generate summary using OpenAI."""
        prompt = f"""Create a concise summary of this AI assistant response.
        
Requirements:
- Maximum 500 characters
- Capture key points and actions taken
- Include mentioned technologies/concepts
- Preserve important code references

Response:
{content[:3000]}  # Limit input to save tokens

Summary:"""
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=150,
            temperature=0.3
        )
        
        summary = response.choices[0].message.content.strip()
        
        # Ensure it's not too long
        if len(summary) > 500:
            summary = summary[:497] + "..."
        
        return summary
    
    async def _vectorize(self, text: str) -> list[float]:
        """Generate embedding for text."""
        response = await self.client.embeddings.create(
            model="text-embedding-3-small",
            input=[text]
        )
        
        return response.data[0].embedding
```

### Worker 5: Entity Deduplicator

**Goal:** Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ñ‚Ğ° Ğ¾Ğ±'Ñ”Ğ´Ğ½Ğ°Ñ‚Ğ¸ Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ½Ñ– entities

```python
# backend/app/agents/cursor/workers/entity_deduplicator.py

class EntityDeduplicator:
    """
    ĞĞ±'Ñ”Ğ´Ğ½ÑƒÑ” Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ½Ñ– entities (Ğ½Ğ¸Ğ·ÑŒĞºĞ¸Ğ¹ Ğ¿Ñ€Ñ–Ğ¾Ñ€Ğ¸Ñ‚ĞµÑ‚).
    """
    
    def __init__(self, repository: CursorRepository):
        self.repo = repository
    
    async def run(self):
        """Check ĞºĞ¾Ğ¶Ğ½Ñ– 30 ÑĞµĞºÑƒĞ½Ğ´ (Ñ€Ñ–Ğ´ĞºĞ¾)."""
        logger.info("ğŸ”„ Entity Deduplicator started")
        
        while True:
            try:
                # Find duplicate groups
                duplicates = await self._find_duplicates()
                
                logger.info(f"Found {len(duplicates)} duplicate groups")
                
                for group in duplicates:
                    await self._merge_entity_group(group)
                
                await asyncio.sleep(30)  # Low priority - every 30s
                
            except Exception as e:
                logger.error(f"âŒ Deduplicator error: {e}", exc_info=True)
                await asyncio.sleep(60)
    
    async def _find_duplicates(self) -> list[list[Entity]]:
        """
        Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ Ğ³Ñ€ÑƒĞ¿Ğ¸ Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ½Ğ¸Ñ… entities.
        
        ĞšÑ€Ğ¸Ñ‚ĞµÑ€Ñ–Ñ— Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ñƒ:
        - ĞĞ´Ğ½Ğ°ĞºĞ¾Ğ²Ğ¸Ğ¹ canonical_name + type
        - Ğ”ÑƒĞ¶Ğµ ÑÑ…Ğ¾Ğ¶Ğ¸Ğ¹ name (Levenshtein distance < 2)
        """
        cypher = """
        MATCH (e:Entity)
        WITH e.canonical_name as canonical, e.type as type, collect(e) as entities
        WHERE size(entities) > 1
        RETURN canonical, type, entities
        ORDER BY size(entities) DESC
        """
        
        results = await self.repo.query(cypher)
        
        duplicate_groups = []
        for row in results:
            entities = [Entity(**e) for e in row["entities"]]
            duplicate_groups.append(entities)
        
        return duplicate_groups
    
    async def _merge_entity_group(self, entities: list[Entity]):
        """
        ĞĞ±'Ñ”Ğ´Ğ½Ğ°Ñ‚Ğ¸ Ğ³Ñ€ÑƒĞ¿Ñƒ Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ½Ğ¸Ñ… entities Ğ² Ğ¾Ğ´Ğ¸Ğ½ canonical.
        
        Strategy:
        1. Ğ’Ğ¸Ğ±Ñ€Ğ°Ñ‚Ğ¸ canonical (Ğ½Ğ°Ğ¹Ğ±Ñ–Ğ»ÑŒÑˆĞµ mention_count)
        2. ĞŸĞµÑ€ĞµĞ½ĞµÑÑ‚Ğ¸ Ğ²ÑÑ– relationships Ğ½Ğ° canonical
        3. Ğ”Ğ¾Ğ´Ğ°Ñ‚Ğ¸ aliases
        4. Ğ’Ğ¸Ğ´Ğ°Ğ»Ğ¸Ñ‚Ğ¸ Ğ´ÑƒĞ±Ğ»Ñ–ĞºĞ°Ñ‚Ğ¸
        """
        # Choose canonical (most mentioned)
        canonical = max(entities, key=lambda e: e.mention_count)
        
        logger.info(
            f"ğŸ”„ Merging {len(entities)} entities into: {canonical.name}"
        )
        
        # Collect all names as aliases
        all_names = set(e.name for e in entities)
        all_names.discard(canonical.name)  # Remove canonical from aliases
        
        # Merge each duplicate into canonical
        for duplicate in entities:
            if duplicate.id != canonical.id:
                # Transfer all MENTIONS relationships
                cypher_transfer = """
                MATCH (duplicate:Entity {id: $dup_id})<-[r:MENTIONS]-(source)
                MATCH (canonical:Entity {id: $can_id})
                CREATE (source)-[new:MENTIONS]->(canonical)
                SET new = properties(r)
                DELETE r
                """
                
                await self.repo.execute(cypher_transfer, {
                    "dup_id": duplicate.id,
                    "can_id": canonical.id
                })
                
                # Delete duplicate
                await self.repo.delete_entity(duplicate.id)
                
                logger.info(f"  â†’ Merged: {duplicate.name}")
        
        # Update canonical with aliases
        await self.repo.update_entity_aliases(
            entity_id=canonical.id,
            aliases=list(all_names)
        )
        
        # Recalculate mention_count
        new_count = await self.repo.count_entity_mentions(canonical.id)
        await self.repo.update_entity_mention_count(canonical.id, new_count)
        
        logger.info(
            f"âœ… Merged into {canonical.name} "
            f"(mentions: {new_count}, aliases: {len(all_names)})"
        )
```

### Worker 6: Git Change Watcher

**Goal:** Ğ’Ñ–Ğ´ÑÑ‚ĞµĞ¶ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ·Ğ¼Ñ–Ğ½Ğ¸ Ğ² git Ñ‚Ğ° Ğ¾Ğ½Ğ¾Ğ²Ğ»ÑĞ²Ğ°Ñ‚Ğ¸ code nodes Ğ² Ğ³Ñ€Ğ°Ñ„Ñ–

```python
# backend/app/agents/cursor/workers/git_watcher.py

import subprocess
from pathlib import Path

class GitChangeWatcher:
    """
    Ğ’Ñ–Ğ´ÑÑ‚ĞµĞ¶ÑƒÑ” git commits Ñ‚Ğ° Ğ¾Ğ½Ğ¾Ğ²Ğ»ÑÑ” code structure Ğ² Ğ³Ñ€Ğ°Ñ„Ñ–.
    """
    
    def __init__(self, repository: CursorRepository, repo_path: Path):
        self.repo = repository
        self.repo_path = repo_path
    
    async def run(self):
        """Check ĞºĞ¾Ğ¶Ğ½Ñƒ Ñ…Ğ²Ğ¸Ğ»Ğ¸Ğ½Ñƒ."""
        logger.info("ğŸ” Git Change Watcher started")
        
        while True:
            try:
                # Get last processed commit
                last_commit = await self.repo.get_last_processed_commit()
                
                # Get new commits
                new_commits = await self._get_new_commits(last_commit)
                
                if new_commits:
                    logger.info(f"ğŸ“ Processing {len(new_commits)} new commits...")
                    
                    for commit in new_commits:
                        await self._process_commit(commit)
                
                await asyncio.sleep(60)  # Every minute
                
            except Exception as e:
                logger.error(f"âŒ Git Watcher error: {e}", exc_info=True)
                await asyncio.sleep(120)
    
    async def _get_new_commits(self, since_commit: str | None) -> list[GitCommit]:
        """Get commits since last processed."""
        cmd = ["git", "log", "--oneline", "--no-merges"]
        
        if since_commit:
            cmd.append(f"{since_commit}..HEAD")
        else:
            cmd.extend(["--max-count=10"])  # First run - last 10 commits
        
        result = subprocess.run(
            cmd,
            cwd=self.repo_path,
            capture_output=True,
            text=True
        )
        
        commits = []
        for line in result.stdout.strip().split("\n"):
            if line:
                sha, message = line.split(" ", 1)
                commits.append(GitCommit(sha=sha, message=message))
        
        return commits
    
    async def _process_commit(self, commit: GitCommit):
        """Process single commit."""
        logger.info(f"ğŸ“ Processing commit {commit.sha[:7]}: {commit.message}")
        
        # Get changed files
        cmd = ["git", "show", "--name-status", "--pretty=", commit.sha]
        result = subprocess.run(
            cmd,
            cwd=self.repo_path,
            capture_output=True,
            text=True
        )
        
        changes = []
        for line in result.stdout.strip().split("\n"):
            if line:
                status, path = line.split("\t", 1)
                changes.append(FileChange(status=status, path=path))
        
        # Process each change
        for change in changes:
            if change.path.endswith(".py"):
                await self._process_file_change(commit, change)
        
        # Create Commit node
        await self.repo.create_commit_node(
            sha=commit.sha,
            message=commit.message,
            timestamp=datetime.now(),
            files_changed=len(changes)
        )
        
        # Mark as processed
        await self.repo.mark_commit_processed(commit.sha)
    
    async def _process_file_change(self, commit: GitCommit, change: FileChange):
        """Process Python file change."""
        if change.status == "D":
            # File deleted - mark nodes as invalid
            await self.repo.mark_code_nodes_invalid(
                file_path=change.path,
                invalid_at=datetime.now(),
                reason=f"Deleted in {commit.sha[:7]}"
            )
            logger.info(f"  âŒ Marked deleted: {change.path}")
        
        elif change.status in ["A", "M"]:
            # File added or modified - reparse
            file_path = self.repo_path / change.path
            
            if file_path.exists():
                content = file_path.read_text(encoding="utf-8")
                
                # Parse Python AST
                import ast
                tree = ast.parse(content)
                
                # Extract functions
                for node in ast.walk(tree):
                    if isinstance(node, ast.FunctionDef):
                        func_source = ast.get_source_segment(content, node)
                        
                        # Create or update Function node
                        await self.repo.create_or_update_function(
                            name=node.name,
                            file_path=change.path,
                            source_code=func_source,
                            start_line=node.lineno,
                            end_line=node.end_lineno,
                            commit_sha=commit.sha,
                            valid_at=datetime.now()
                        )
                
                logger.info(f"  âœ… Updated: {change.path}")
```

---

## Enhanced Graph Schema

### Complete Node Types

```cypher
// ============================================
// KNOWLEDGE BASE (Rules, Code, Docs)
// ============================================

(:KnowledgeBase {
  id: "cursor_kb_v1",
  version: "1.0.0",
  initialized_at: "ISO8601",
  total_documents: 50,
  total_chunks: 2500,
  total_code_files: 120,
  total_entities: 350
})

(:Document {
  id: "uuid",
  path: ".cursor/rules/agents/clerk.mdc",
  type: "rules|code|docs",
  content: "full text",
  content_hash: "sha256",
  loaded_at: "ISO8601",
  last_modified: "ISO8601",
  chunk_count: 15,
  status: "active|archived"
})

(:Chunk {
  id: "uuid",
  content: "chunk text (max 800 chars)",
  position: 0,
  char_start: 0,
  char_end: 800,
  chunk_type: "paragraph|sentence|code|heading",
  
  // Embedding (Ğ´Ğ¾Ğ´Ğ°Ñ”Ñ‚ÑŒÑÑ worker)
  embedding: [float Ã— 1536],
  embedding_model: "text-embedding-3-small",
  embedding_created_at: "ISO8601",
  
  // Processing status
  status: "pending_vectorization|vectorized|similarity_checked",
  
  // Temporal
  valid_at: "ISO8601",
  invalid_at: "ISO8601|NULL"
})

// ============================================
// CODE STRUCTURE
// ============================================

(:CodeFile {
  id: "uuid",
  path: "backend/app/agents/clerk/nodes.py",
  language: "python",
  lines_count: 150,
  content_hash: "sha256",
  
  // Temporal
  valid_at: "ISO8601",
  invalid_at: "ISO8601|NULL",
  
  // Git context
  commit_sha: "abc123",
  last_modified: "ISO8601"
})

(:Function {
  id: "uuid",
  name: "clerk_record_node",
  signature: "async def clerk_record_node(state: dict, repository: MessageRepository) -> dict",
  source_code: "full function code",
  start_line: 10,
  end_line: 50,
  
  // Embedding
  embedding: [float Ã— 1536],
  
  // Metadata
  is_async: true,
  parameters: ["state", "repository"],
  return_type: "dict",
  
  // Temporal
  valid_at: "ISO8601",
  invalid_at: "ISO8601|NULL"
})

(:Class {
  id: "uuid",
  name: "CursorRepository",
  docstring: "Repository for cursor_memory operations",
  methods: ["create_session", "create_user_query"],
  inherits_from: ["BaseRepository"],
  
  // Embedding
  embedding: [float Ã— 1536]
})

(:Import {
  id: "uuid",
  module: "fastapi",
  imported_names: ["FastAPI", "HTTPException"],
  is_from_import: true
})

// ============================================
// GIT HISTORY
// ============================================

(:Commit {
  sha: "abc123def456",
  message: "Add clerk agent implementation",
  author: "developer",
  timestamp: "ISO8601",
  files_changed: 5
})

// ============================================
// USER INTERACTIONS (Runtime)
// ============================================

(:UserQuery {
  id: "uuid",
  content: "full query text",
  timestamp: "ISO8601",
  
  // Embedding (Ğ´Ğ¾Ğ´Ğ°Ñ”Ñ‚ÑŒÑÑ worker)
  embedding: [float Ã— 1536],
  
  // Metadata
  content_length: 250,
  has_code: false,
  language: "uk",
  
  // Processing status
  status: "pending_vectorization|vectorized|entities_extracted",
  
  // Temporal
  valid_at: "ISO8601",
  invalid_at: "ISO8601|NULL"
})

(:AssistantResponse {
  id: "uuid",
  
  // Short summary (Ğ·Ğ°Ğ²Ğ¶Ğ´Ğ¸ Ğ² Ğ³Ñ€Ğ°Ñ„Ñ–)
  summary: "Short summary max 500 chars",
  summary_embedding: [float Ã— 1536],
  
  // Full content reference (ÑĞºÑ‰Ğ¾ >1000 chars)
  full_content_ref: "responses/2025/11/13/uuid.md",
  content_length: 5000,
  
  // Metadata
  has_code_blocks: true,
  code_blocks_count: 3,
  tools_used: ["read_file", "codebase_search"],
  files_modified: ["backend/app/main.py"],
  processing_time_ms: 2534,
  
  // Processing status
  status: "pending_summarization|summarized|vectorized",
  
  // Temporal
  timestamp: "ISO8601",
  valid_at: "ISO8601"
})

(:CodeBlock {
  id: "uuid",
  language: "python",
  content: "code snippet",
  position: 0,  // Position in response
  lines_count: 15,
  
  // Embedding
  embedding: [float Ã— 1536]
})

// ============================================
// ENTITIES (Cross-cutting)
// ============================================

(:Entity {
  id: "uuid",
  
  // Names
  name: "Docker",  // Original form
  canonical_name: "docker",  // Normalized
  aliases: ["docker", "Docker Engine", "docker-compose"],
  
  // Classification
  type: "TECH|PERSON|ORG|LOCATION|CONCEPT|EVENT|COMPONENT",
  
  // Statistics
  mention_count: 45,
  first_seen: "ISO8601",
  last_seen: "ISO8601",
  
  // Embedding (Ğ´Ğ»Ñ entity search)
  embedding: [float Ã— 1536],
  
  // Confidence
  confidence: 0.95,
  
  // Status
  status: "active|merged|deprecated"
})

// ============================================
// ARCHITECTURAL DECISIONS (Future Phase)
// ============================================

(:ArchitecturalDecision {
  id: "uuid",
  title: "Switch to async-first architecture",
  description: "All I/O operations must be async",
  rationale: "Better performance and scalability",
  alternatives: ["sync with threading", "celery workers"],
  decided_at: "ISO8601",
  decided_by: "user|ai|collaborative",
  status: "proposed|accepted|implemented|deprecated"
})

(:Issue {
  id: "uuid",
  title: "FalkorDB datetime() not supported",
  description: "Unknown function error when using datetime()",
  root_cause: "FalkorDB doesn't support datetime() in Cypher",
  solution: "Store timestamps as ISO strings",
  discovered_at: "ISO8601",
  resolved_at: "ISO8601|NULL",
  status: "encountered|investigating|resolved|unresolved",
  severity: "critical|high|medium|low"
})
```

### Complete Relationship Types

```cypher
// ============================================
// KNOWLEDGE BASE RELATIONSHIPS
// ============================================

(:Document)-[:IN_BASE]->(:KnowledgeBase)
(:Chunk)-[:PART_OF {position: int}]->(:Document)
(:CodeFile)-[:IN_BASE]->(:KnowledgeBase)
(:Function)-[:IN_FILE {start_line: int, end_line: int}]->(:CodeFile)
(:Class)-[:IN_FILE]->(:CodeFile)
(:Function)-[:METHOD_OF]->(:Class)
(:Import)-[:IN_FILE]->(:CodeFile)

// ============================================
// GIT RELATIONSHIPS
// ============================================

(:Commit)-[:MODIFIED]->(:CodeFile)
(:Commit)-[:ADDED]->(:CodeFile)
(:Commit)-[:DELETED]->(:CodeFile)
(:UserQuery)-[:CAUSED_CHANGES_IN]->(:Commit)

// ============================================
// USER INTERACTION RELATIONSHIPS
// ============================================

(:AssistantResponse)-[:ANSWERS]->(:UserQuery)
(:CodeBlock)-[:PART_OF {position: int}]->(:AssistantResponse)

// ============================================
// ENTITY RELATIONSHIPS
// ============================================

// Query/Response mentions entities
(:UserQuery)-[:MENTIONS {
  position: int,
  context: "surrounding text",
  confidence: float
}]->(:Entity)

(:AssistantResponse)-[:MENTIONS]->(:Entity)

// Knowledge base mentions entities
(:Chunk)-[:MENTIONS]->(:Entity)
(:Function)-[:MENTIONS]->(:Entity)
(:Document)-[:DISCUSSES {
  salience: float,  // Ğ’Ğ°Ğ¶Ğ»Ğ¸Ğ²Ñ–ÑÑ‚ÑŒ entity Ğ² document
  mention_count: int
}]->(:Entity)

// Entity co-occurrence
(:Entity)-[:RELATED_TO {
  co_occurrence_count: int,
  strength: float,
  contexts: ["query_id1", "query_id2"]
}]-(:Entity)

// Entity merging history
(:Entity)-[:MERGED_INTO {
  merged_at: "ISO8601",
  reason: "duplicate"
}]->(:Entity)

// ============================================
// SIMILARITY RELATIONSHIPS
// ============================================

// Semantic similarity (Ñ‡ĞµÑ€ĞµĞ· embeddings)
(:Chunk)-[:SIMILAR_TO {
  similarity: float,
  algorithm: "cosine",
  created_at: "ISO8601"
}]-(:Chunk)

(:UserQuery)-[:SIMILAR_TO]->(:UserQuery)
(:Function)-[:SIMILAR_TO]->(:Function)

// Message semantic relation
(:UserQuery)-[:SEMANTICALLY_RELATED {
  similarity: float,
  via_entities: ["entity_id1", "entity_id2"],
  via_chunks: ["chunk_id1", "chunk_id2"]
}]->(:UserQuery)

// ============================================
// TEMPORAL RELATIONSHIPS
// ============================================

(:UserQuery)-[:FOLLOWED_BY {
  time_delta_seconds: int
}]->(:UserQuery)

// ============================================
// DECISION RELATIONSHIPS (Future Phase)
// ============================================

(:UserQuery)-[:LEADS_TO]->(:ArchitecturalDecision)
(:ArchitecturalDecision)-[:AFFECTS]->(:Component)
(:Issue)-[:DISCOVERED_BY]->(:UserQuery)
(:Issue)-[:FIXED_BY]->(:ArchitecturalDecision)
(:Issue)-[:FOUND_IN]->(:CodeFile)
```

---

## Entity Deduplication Strategy

### Problem: Multiple Names for Same Thing

```
"Docker" vs "docker" vs "Docker Engine" vs "docker-compose"
"FalkorDB" vs "falkordb" vs "FalkorDB graph database"
"Clerk Agent" vs "clerk" vs "ĞŸĞ¸ÑĞ°Ñ€ÑŒ"
```

### Solution: Multi-Stage Deduplication

#### Stage 1: Normalization (During Extraction)

```python
def normalize_entity(name: str, entity_type: str) -> str:
    """
    Normalize entity name Ğ´Ğ»Ñ canonical_name.
    
    Rules:
    1. Lowercase
    2. Trim whitespace
    3. Remove version numbers (Ğ´Ğ»Ñ TECH)
    4. Apply known aliases
    """
    canonical = name.lower().strip()
    
    # Remove version numbers
    if entity_type == "TECH":
        canonical = re.sub(r'\s+v?\d+(\.\d+)*', '', canonical)
        canonical = re.sub(r'\s+\d+(\.\d+)*', '', canonical)
    
    # Known aliases
    ALIASES = {
        "k8s": "kubernetes",
        "js": "javascript",
        "ts": "typescript",
        "py": "python",
        "gpt": "chatgpt",
        "ai": "artificial_intelligence",
    }
    
    canonical = ALIASES.get(canonical, canonical)
    
    return canonical
```

#### Stage 2: Fuzzy Matching (During Extraction)

```python
async def find_similar_entity(
    canonical_name: str,
    entity_type: str
) -> Entity | None:
    """
    Find existing entity using fuzzy matching.
    
    Methods:
    1. Exact match on canonical_name
    2. Check aliases
    3. Substring match
    4. Levenshtein distance < 2
    """
    # Try exact match
    cypher = """
    MATCH (e:Entity {canonical_name: $canonical, type: $type})
    RETURN e, 1.0 as similarity
    """
    
    result = await repo.query(cypher, {
        "canonical": canonical_name,
        "type": entity_type
    })
    
    if result:
        return result[0]
    
    # Try alias match
    cypher = """
    MATCH (e:Entity {type: $type})
    WHERE $canonical IN e.aliases
    RETURN e, 0.95 as similarity
    """
    
    result = await repo.query(cypher, {
        "canonical": canonical_name,
        "type": entity_type
    })
    
    if result:
        return result[0]
    
    # Try substring match (one contains another)
    cypher = """
    MATCH (e:Entity {type: $type})
    WHERE e.canonical_name CONTAINS $canonical
       OR $canonical CONTAINS e.canonical_name
    RETURN e, 0.8 as similarity
    ORDER BY length(e.canonical_name) ASC
    LIMIT 1
    """
    
    result = await repo.query(cypher, {
        "canonical": canonical_name,
        "type": entity_type
    })
    
    if result:
        return result[0]
    
    # No match found
    return None
```

#### Stage 3: Background Deduplication (Worker)

Deduplicator worker Ğ·Ğ½Ğ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ÑŒ entities Ğ·:
- ĞĞ´Ğ½Ğ°ĞºĞ¾Ğ²Ğ¸Ğ¼ `canonical_name + type`
- Levenshtein distance < 2
- High embedding similarity (cosine > 0.95)

```python
async def merge_entities(canonical_id: str, duplicate_ids: list[str]):
    """
    Merge duplicate entities into canonical.
    
    Process:
    1. Transfer all MENTIONS relationships
    2. Collect all names as aliases
    3. Sum mention_counts
    4. Update first_seen / last_seen
    5. Delete duplicates
    """
    # Get canonical entity
    canonical = await repo.get_entity(canonical_id)
    
    # Collect aliases
    all_aliases = set(canonical.aliases or [])
    total_mentions = canonical.mention_count
    earliest_seen = canonical.first_seen
    latest_seen = canonical.last_seen
    
    for dup_id in duplicate_ids:
        duplicate = await repo.get_entity(dup_id)
        
        # Add name and aliases
        all_aliases.add(duplicate.name)
        if duplicate.aliases:
            all_aliases.update(duplicate.aliases)
        
        # Update statistics
        total_mentions += duplicate.mention_count
        earliest_seen = min(earliest_seen, duplicate.first_seen)
        latest_seen = max(latest_seen, duplicate.last_seen)
        
        # Transfer relationships
        cypher = """
        MATCH (duplicate:Entity {id: $dup_id})<-[r:MENTIONS]-(source)
        MATCH (canonical:Entity {id: $can_id})
        MERGE (source)-[new:MENTIONS]->(canonical)
        ON CREATE SET new = properties(r)
        DELETE r
        """
        
        await repo.execute(cypher, {
            "dup_id": dup_id,
            "can_id": canonical_id
        })
        
        # Create merge history
        await repo.create_merge_relationship(
            from_id=dup_id,
            to_id=canonical_id,
            merged_at=datetime.now(),
            reason="duplicate"
        )
        
        # Delete duplicate
        await repo.delete_entity(dup_id)
    
    # Update canonical
    await repo.update_entity(
        entity_id=canonical_id,
        aliases=list(all_aliases),
        mention_count=total_mentions,
        first_seen=earliest_seen,
        last_seen=latest_seen
    )
    
    logger.info(
        f"âœ… Merged {len(duplicate_ids)} entities into {canonical.name} "
        f"(total mentions: {total_mentions}, aliases: {len(all_aliases)})"
    )
```

---

## Code Evolution Tracking

### Goal: ĞŸĞ¾Ğ²'ÑĞ·Ğ°Ñ‚Ğ¸ ĞºĞ¾Ğ´ Ğ· Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ğ¼Ğ¸ Ñ‚Ğ° Ñ€Ñ–ÑˆĞµĞ½Ğ½ÑĞ¼Ğ¸

**Use Cases:**
- "Ğ¯ĞºÑ– Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ¸ Ğ¿Ñ€Ğ¸Ğ·Ğ²ĞµĞ»Ğ¸ Ğ´Ğ¾ ÑÑ‚Ğ²Ğ¾Ñ€ĞµĞ½Ğ½Ñ Ñ†Ñ–Ñ”Ñ— Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ—?"
- "ĞšĞ¾Ğ»Ğ¸ Ğ¼Ğ¸ Ğ²Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ” Ğ·Ğ¼Ñ–Ğ½ÑĞ²Ğ°Ğ»Ğ¸ Clerk Agent?"
- "Ğ¯ĞºÑ– Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ğ¸ Ğ²Ğ¸Ğ½Ğ¸ĞºĞ°Ğ»Ğ¸ Ğ¿Ñ€Ğ¸ Ñ–Ğ¼Ğ¿Ğ»ĞµĞ¼ĞµĞ½Ñ‚Ğ°Ñ†Ñ–Ñ— Repository pattern?"
- "Ğ¯Ğº ĞµĞ²Ğ¾Ğ»ÑÑ†Ñ–Ğ¾Ğ½ÑƒĞ²Ğ°Ğ² Ñ†ĞµĞ¹ Ğ¼Ğ¾Ğ´ÑƒĞ»ÑŒ Ğ·Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ– 3 Ğ¼Ñ–ÑÑÑ†Ñ–?"

### Implementation

#### 1. Initial Codebase Index (Ğ¿Ñ€Ğ¸ ÑÑ‚Ğ°Ñ€Ñ‚Ñ–)

```python
async def index_codebase():
    """Parse existing codebase and create initial nodes."""
    python_files = glob.glob("backend/**/*.py", recursive=True)
    
    for file_path in python_files:
        # Get git history for file
        commits = await git.get_file_history(file_path)
        first_commit = commits[-1] if commits else None
        
        # Parse current version
        content = Path(file_path).read_text()
        tree = ast.parse(content)
        
        # Create CodeFile node
        file_id = await repo.create_code_file(
            path=file_path,
            language="python",
            content_hash=hash_content(content),
            valid_at=first_commit.timestamp if first_commit else datetime.now()
        )
        
        # Extract functions
        for node in ast.walk(tree):
            if isinstance(node, ast.FunctionDef):
                await repo.create_function(
                    name=node.name,
                    file_id=file_id,
                    source_code=ast.get_source_segment(content, node),
                    # ... metadata
                )
```

#### 2. Link Queries to Commits

**Strategy:** ĞšĞ¾Ğ»Ğ¸ ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ Ñ‰Ğ¾ÑÑŒ Ğ·Ğ¼Ñ–Ğ½ÑÑ”, Ğ¼Ğ¸ Ğ¿Ğ¾Ğ²'ÑĞ·ÑƒÑ”Ğ¼Ğ¾ Ğ¹Ğ¾Ğ³Ğ¾ Ğ·Ğ°Ğ¿Ğ¸Ñ‚ Ğ· Ğ½Ğ°ÑÑ‚ÑƒĞ¿Ğ½Ğ¸Ğ¼ commit.

```python
# Ğ’ Hot Path (Ğ¿Ñ–ÑĞ»Ñ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–)
if response.contains_file_edits:
    # Mark query as "pending_commit_link"
    await repo.mark_query_pending_commit_link(query_id)

# Git Watcher (ĞºĞ¾Ğ»Ğ¸ Ğ±Ğ°Ñ‡Ğ¸Ñ‚ÑŒ Ğ½Ğ¾Ğ²Ğ¸Ğ¹ commit)
async def process_new_commit(commit: GitCommit):
    # Ğ—Ğ½Ğ°Ğ¹Ñ‚Ğ¸ queries pending link Ğ·Ğ° Ğ¾ÑÑ‚Ğ°Ğ½Ğ½Ñ– 10 Ñ…Ğ²Ğ¸Ğ»Ğ¸Ğ½
    pending_queries = await repo.get_queries_pending_commit_link(
        since=commit.timestamp - timedelta(minutes=10)
    )
    
    for query in pending_queries:
        # Create relationship
        await repo.create_relationship(
            from_node=query.id,
            to_node=commit.sha,
            type="CAUSED_CHANGES_IN",
            properties={
                "time_delta_seconds": (commit.timestamp - query.timestamp).total_seconds()
            }
        )
        
        logger.info(f"ğŸ”— Linked query {query.id[:8]} to commit {commit.sha[:7]}")
```

#### 3. Track Function Evolution

```cypher
// Query: Ğ¯Ğº ĞµĞ²Ğ¾Ğ»ÑÑ†Ñ–Ğ¾Ğ½ÑƒĞ²Ğ°Ğ»Ğ° Ñ„ÑƒĞ½ĞºÑ†Ñ–Ñ clerk_record_node?
MATCH (f:Function {name: "clerk_record_node"})
OPTIONAL MATCH (f)-[:IN_FILE]->(file:CodeFile)<-[:MODIFIED]-(c:Commit)
OPTIONAL MATCH (c)<-[:CAUSED_CHANGES_IN]-(q:UserQuery)
RETURN f, file, c, q
ORDER BY c.timestamp ASC

// Result: Ğ¥Ñ€Ğ¾Ğ½Ğ¾Ğ»Ğ¾Ğ³Ñ–Ñ Ğ·Ğ¼Ñ–Ğ½ Ğ· Ğ¿Ğ¾Ğ²'ÑĞ·Ğ°Ğ½Ğ¸Ğ¼Ğ¸ Ğ·Ğ°Ğ¿Ğ¸Ñ‚Ğ°Ğ¼Ğ¸
```

#### 4. Code Reference in Context

```python
async def build_context_with_code(query: UserQuery):
    """Include relevant code in context."""
    # Find entities
    entities = await find_entities_in_query(query)
    
    # Find code nodes mentioning these entities
    code_context = []
    for entity in entities:
        # Functions that mention this entity
        functions = await repo.get_functions_mentioning_entity(entity.id)
        
        for func in functions:
            code_context.append({
                "type": "function",
                "name": func.name,
                "file": func.file_path,
                "source": func.source_code,
                "relevance": "mentions " + entity.name
            })
    
    return {
        "entities": entities,
        "code_context": code_context,
        # ... other context
    }
```

---

## Response Storage Strategy

### Problem: ĞœĞ¾Ñ— Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ¼Ğ¾Ğ¶ÑƒÑ‚ÑŒ Ğ±ÑƒÑ‚Ğ¸ Ğ´ÑƒĞ¶Ğµ Ğ´Ğ¾Ğ²Ğ³Ğ¸Ğ¼Ğ¸

- ĞŸÑ€Ğ¾ÑÑ‚Ñ– Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–: 200-500 chars
- Ğ¡ĞµÑ€ĞµĞ´Ğ½Ñ–: 1000-2000 chars
- Ğ”Ğ¾Ğ²Ğ³Ñ–: 3000-10000 chars
- Ğ— ĞºĞ¾Ğ´Ğ¾Ğ¼: Ğ¼Ğ¾Ğ¶Ğµ Ğ±ÑƒÑ‚Ğ¸ 20000+ chars

### Solution: Hybrid Storage

#### Short Responses (<1000 chars)

**Ğ—Ğ±ĞµÑ€Ñ–Ğ³Ğ°Ñ”Ğ¼Ğ¾ Ğ¿Ğ¾Ğ²Ğ½Ñ–ÑÑ‚Ñ Ğ² Ğ³Ñ€Ğ°Ñ„Ñ–:**

```cypher
(:AssistantResponse {
  id: "uuid",
  content: "full text here",
  content_length: 450,
  embedding: [...]
})
```

#### Long Responses (>1000 chars)

**Summary Ğ² Ğ³Ñ€Ğ°Ñ„Ñ– + full text Ğ² Ñ„Ğ°Ğ¹Ğ»Ñ–:**

```cypher
(:AssistantResponse {
  id: "uuid",
  summary: "Created Clerk Agent implementation with async repository pattern...",
  summary_embedding: [...],
  full_content_ref: "responses/2025/11/13/abc123.md",
  content_length: 5000
})
```

**File structure:**
```
responses/
  2025/
    11/
      13/
        abc123def456.md  # Response ID as filename
        abc456def789.md
```

**File format:**
```markdown
---
response_id: abc123def456
query_id: query789
timestamp: 2025-11-13T15:30:00Z
content_length: 5000
---

# Assistant Response

[Full response content here...]
```

#### Code Blocks (Ğ¾ĞºÑ€ĞµĞ¼Ñ– nodes)

```cypher
(:AssistantResponse)-[:CONTAINS_CODE {position: 0}]->(:CodeBlock {
  language: "python",
  content: "async def example():\n    pass",
  lines_count: 2,
  embedding: [...]
})
```

**Ğ§Ğ¾Ğ¼Ñƒ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾?**
- ĞœĞ¾Ğ¶Ğ½Ğ° ÑˆÑƒĞºĞ°Ñ‚Ğ¸ code snippets Ğ½ĞµĞ·Ğ°Ğ»ĞµĞ¶Ğ½Ğ¾
- ĞœĞ¾Ğ¶Ğ½Ğ° Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾
- ĞœĞ¾Ğ¶Ğ½Ğ° Ğ·Ğ²'ÑĞ·ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ· Functions (ÑÑ…Ğ¾Ğ¶Ñ–ÑÑ‚ÑŒ)

### Indexing by Cursor

```python
async def index_response_file(response_id: str, file_path: Path):
    """
    Cursor Ğ¼Ğ¾Ğ¶Ğµ Ğ¿Ñ€Ğ¾Ñ–Ğ½Ğ´ĞµĞºÑÑƒĞ²Ğ°Ñ‚Ğ¸ Ñ„Ğ°Ğ¹Ğ» Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–.
    
    - Read metadata from frontmatter
    - Link to query
    - Update search index
    """
    # Cursor's file indexer picks this up automatically
    # since it's in workspace
    pass
```

---

## Performance Targets & SLA

### Hot Path (User Blocking)

| Operation | Target | Max | Notes |
|-----------|--------|-----|-------|
| Save UserQuery | 10-20ms | 30ms | Single CREATE |
| Quick Entity Match | 20-40ms | 60ms | Substring search |
| Context Retrieval | 30-50ms | 80ms | Cypher with JOINs |
| Save Response | 15-25ms | 40ms | Single CREATE |
| **TOTAL OVERHEAD** | **75-135ms** | **210ms** | Before Gemini |
| Gemini Response | 2-3s | 5s | External API |
| **TOTAL USER WAIT** | **2.1-3.1s** | **5.2s** | End-to-end |

### Async Workers (Non-Blocking)

| Worker | Processing Time | Frequency | Notes |
|--------|----------------|-----------|-------|
| Real-time Vectorizer | 150-250ms | 0.1s | OpenAI API |
| Entity Extractor | 300-500ms | 1s | OpenAI + fuzzy match |
| Similarity Linker | 50-150ms | 2s | In-memory NumPy |
| Response Summarizer | 400-800ms | 5s | OpenAI + file I/O |
| Entity Deduplicator | 100-300ms | 30s | Graph queries |
| Git Change Watcher | 200-500ms | 60s | Git commands + parsing |

### Database Performance

| Query Type | Target | Max | Optimization |
|------------|--------|-----|--------------|
| Entity substring match | 10-20ms | 40ms | Index on canonical_name |
| Get recent messages | 20-30ms | 50ms | Index on timestamp |
| Similarity search (1K nodes) | 40-80ms | 150ms | In-memory NumPy |
| Similarity search (10K nodes) | 80-200ms | 400ms | Consider Redis VSS |
| Similarity search (100K+ nodes) | N/A | N/A | MUST use Redis/Qdrant |

### Scaling Thresholds

**Ğ”Ğ¾ 10,000 queries:**
- âœ… In-memory similarity Ğ¿Ñ€Ğ°Ñ†ÑÑ”
- âœ… Single FalkorDB instance
- âœ… All workers Ğ½Ğ° Ğ¾Ğ´Ğ½Ñ–Ğ¹ machine

**10,000 - 50,000 queries:**
- âš ï¸ In-memory similarity Ğ¿Ğ¾Ğ²Ñ–Ğ»ÑŒĞ½Ñ–Ñ”
- ğŸ”§ Consider Redis Stack VSS
- âœ… Single instance Ñ‰Ğµ OK

**50,000+ queries:**
- âŒ MUST use vector DB (Redis/Qdrant)
- ğŸ”§ Consider horizontal scaling workers
- ğŸ”§ FalkorDB Ğ¼Ğ¾Ğ¶ĞµÑ‚ Ğ¿Ğ¾Ñ‚Ñ€ĞµĞ±ÑƒĞ²Ğ°Ñ‚Ğ¸ Ğ±Ñ–Ğ»ÑŒÑˆĞµ RAM

---

## Implementation Phase Plan

### Phase 1: Foundation (Week 1-2)

**Goal:** Hot path Ğ¿Ñ€Ğ°Ñ†ÑÑ”, ĞºĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ· ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ğ¾Ğ¼

**Tasks:**
- [ ] Enhanced CursorRepository Ğ· entity operations
- [ ] Quick entity matcher (substring search)
- [ ] Context builder Ğ· entity-based retrieval
- [ ] Base knowledge initialization script
- [ ] Load `.cursor/rules/*.mdc` Ğ² Ğ³Ñ€Ğ°Ñ„
- [ ] Entity extraction Ğ· base knowledge

**Deliverables:**
- ĞšĞ¾Ñ€Ğ¸ÑÑ‚ÑƒĞ²Ğ°Ñ‡ Ğ¾Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑ” Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´ÑŒ <3s
- ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ±ÑƒĞ´ÑƒÑ”Ñ‚ÑŒÑÑ Ñ‡ĞµÑ€ĞµĞ· entities
- Base knowledge Ğ² Ğ³Ñ€Ğ°Ñ„Ñ–

**Testing:**
- Query: "Ğ¯Ğº Ğ¿Ñ€Ğ°Ñ†ÑÑ” Clerk Agent?"
- Expected: ĞšĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ· `.cursor/rules/agents/clerk.mdc`

### Phase 2: Async Workers Core (Week 3-4)

**Goal:** Background processing Ğ¿Ñ€Ğ°Ñ†ÑÑ”, Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ°Ñ†Ñ–Ñ Ñ‚Ğ° entity extraction

**Tasks:**
- [ ] Worker 1: Real-time Vectorizer
- [ ] Worker 2: Entity Extractor
- [ ] Worker 3: Similarity Linker
- [ ] Worker orchestration (supervisor)
- [ ] Health monitoring
- [ ] Error handling & retry logic

**Deliverables:**
- Queries Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·ÑƒÑÑ‚ÑŒÑÑ <200ms Ğ¿Ñ–ÑĞ»Ñ Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ–
- Entities Ğ²Ğ¸Ñ‚ÑĞ³ÑƒÑÑ‚ÑŒÑÑ <500ms
- Similarity edges ÑÑ‚Ğ²Ğ¾Ñ€ÑÑÑ‚ÑŒÑÑ <2s

**Testing:**
- Submit query â†’ Check DB after 3s â†’ Verify embedding present
- Submit query â†’ Check DB after 2s â†’ Verify entities extracted
- Submit query â†’ Check DB after 5s â†’ Verify SIMILAR_TO edges

### Phase 3: Response Storage (Week 5)

**Goal:** Ğ”Ğ¾Ğ²Ğ³Ñ– Ğ²Ñ–Ğ´Ğ¿Ğ¾Ğ²Ñ–Ğ´Ñ– Ğ·Ğ±ĞµÑ€Ñ–Ğ³Ğ°ÑÑ‚ÑŒÑÑ ĞµÑ„ĞµĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾

**Tasks:**
- [ ] Worker 4: Response Summarizer
- [ ] File storage structure
- [ ] Code block extraction
- [ ] Summary vectorization
- [ ] File indexing integration

**Deliverables:**
- Responses >1000 chars â†’ summary Ğ² Ğ³Ñ€Ğ°Ñ„Ñ– + file
- Code blocks ÑĞº Ğ¾ĞºÑ€ĞµĞ¼Ñ– nodes
- Summaries Ğ²ĞµĞºÑ‚Ğ¾Ñ€Ğ¸Ğ·Ğ¾Ğ²Ğ°Ğ½Ñ–

**Testing:**
- Generate long response â†’ Verify file created
- Verify summary in graph
- Verify code blocks extracted

### Phase 4: Code Integration (Week 6-7)

**Goal:** Codebase Ğ² Ğ³Ñ€Ğ°Ñ„Ñ–, evolution tracking

**Tasks:**
- [ ] Codebase parser (Python AST)
- [ ] Worker 6: Git Change Watcher
- [ ] Link queries to commits
- [ ] Function evolution tracking
- [ ] Code context Ğ² context builder

**Deliverables:**
- Codebase Ğ¿Ñ€Ğ¾Ñ–Ğ½Ğ´ĞµĞºÑĞ¾Ğ²Ğ°Ğ½Ğ¸Ğ¹
- Git changes Ğ²Ñ–Ğ´ÑÑ‚ĞµĞ¶ÑƒÑÑ‚ÑŒÑÑ
- Queries Ğ¿Ğ¾Ğ²'ÑĞ·Ğ°Ğ½Ñ– Ğ· commits
- Code snippets Ğ² ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚Ñ–

**Testing:**
- Make code change â†’ Commit â†’ Verify linked to query
- Query about function â†’ Verify code included in context
- Check function history â†’ Verify evolution visible

### Phase 5: Entity Intelligence (Week 8)

**Goal:** Smart entity management

**Tasks:**
- [ ] Worker 5: Entity Deduplicator
- [ ] Fuzzy matching improvements
- [ ] Alias management
- [ ] Entity relationship mining
- [ ] Entity-based analytics

**Deliverables:**
- Duplicates Ğ°Ğ²Ñ‚Ğ¾Ğ¼Ğ°Ñ‚Ğ¸Ñ‡Ğ½Ğ¾ Ğ¾Ğ±'Ñ”Ğ´Ğ½ÑƒÑÑ‚ÑŒÑÑ
- Aliases Ğ¿Ñ–Ğ´Ñ‚Ñ€Ğ¸Ğ¼ÑƒÑÑ‚ÑŒÑÑ
- Entity co-occurrence Ğ³Ñ€Ğ°Ñ„Ñ–Ğº

**Testing:**
- Create duplicates â†’ Verify merged after 30s
- Query with alias â†’ Verify recognized
- Check entity relationships â†’ Verify co-occurrence tracked

### Phase 6: Polish & Optimize (Week 9-10)

**Goal:** Production-ready performance

**Tasks:**
- [ ] Performance profiling
- [ ] Query optimization
- [ ] Vector search evaluation (Redis VSS?)
- [ ] Monitoring dashboards
- [ ] Documentation
- [ ] End-to-end testing

**Deliverables:**
- All SLAs met consistently
- Monitoring in place
- Full documentation
- Production deployment ready

**Testing:**
- Load testing (100 queries/hour)
- Measure all timings
- Verify no memory leaks
- Stress test workers

---

## Quick Start Commands

### Initialize Knowledge Base

```bash
# ĞŸĞµÑ€ÑˆĞ¸Ğ¹ Ğ·Ğ°Ğ¿ÑƒÑĞº - Ğ·Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ base knowledge
python -m app.agents.cursor.scripts.init_knowledge_base

# Ğ—Ğ°Ğ²Ğ°Ğ½Ñ‚Ğ°Ğ¶Ğ¸Ñ‚Ğ¸ Ğ¿Ñ€Ğ°Ğ²Ğ¸Ğ»Ğ°
python -m app.agents.cursor.scripts.load_rules

# ĞŸÑ€Ğ¾Ñ–Ğ½Ğ´ĞµĞºÑÑƒĞ²Ğ°Ñ‚Ğ¸ codebase
python -m app.agents.cursor.scripts.index_codebase
```

### Start Workers

```bash
# Ğ’ production Ñ‡ĞµÑ€ĞµĞ· Docker Compose
docker-compose up cursor-workers

# Ğ”Ğ»Ñ development - Ğ·Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ñ‚Ğ¸ Ğ¾ĞºÑ€ĞµĞ¼Ğ¾
python -m app.agents.cursor.workers.vectorizer &
python -m app.agents.cursor.workers.entity_extractor &
python -m app.agents.cursor.workers.similarity_linker &
python -m app.agents.cursor.workers.response_summarizer &
python -m app.agents.cursor.workers.entity_deduplicator &
python -m app.agents.cursor.workers.git_watcher &
```

### Monitor Workers

```bash
# Health check
curl http://localhost:8000/api/cursor/workers/health

# Worker status
curl http://localhost:8000/api/cursor/workers/status

# Processing statistics
curl http://localhost:8000/api/cursor/stats
```

---

## Configuration Reference

```python
# backend/app/core/config.py

class Settings(BaseSettings):
    # ... existing ...
    
    # ===== CURSOR MEMORY SYSTEM =====
    
    # Hot Path Settings
    cursor_hot_path_timeout_ms: int = 200
    cursor_entity_match_method: str = "substring"  # substring|fuzzy|hybrid
    cursor_context_recent_limit: int = 10
    cursor_context_related_limit: int = 5
    cursor_include_code_context: bool = True
    
    # Knowledge Base
    cursor_kb_rules_path: str = ".cursor/rules"
    cursor_kb_codebase_path: str = "backend"
    cursor_kb_docs_path: str = "docs"
    cursor_kb_auto_reload: bool = False
    
    # OpenAI Settings
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    openai_embedding_model: str = "text-embedding-3-small"
    openai_embedding_dimensions: int = 1536
    openai_entity_model: str = "gpt-4o-mini"
    openai_summary_model: str = "gpt-4o-mini"
    
    # Worker Settings
    cursor_vectorizer_frequency_ms: int = 100
    cursor_entity_extractor_frequency_ms: int = 1000
    cursor_similarity_linker_frequency_ms: int = 2000
    cursor_summarizer_frequency_ms: int = 5000
    cursor_deduplicator_frequency_ms: int = 30000
    cursor_git_watcher_frequency_ms: int = 60000
    
    # Worker Batch Sizes
    cursor_vectorizer_batch_size: int = 1
    cursor_entity_extractor_batch_size: int = 3
    cursor_similarity_linker_batch_size: int = 10
    cursor_summarizer_batch_size: int = 5
    
    # Similarity Settings
    cursor_similarity_threshold: float = 0.7
    cursor_similarity_top_k: int = 10
    cursor_similarity_method: str = "cosine"  # cosine|euclidean
    
    # Entity Settings
    cursor_entity_fuzzy_threshold: float = 0.9
    cursor_entity_alias_max: int = 10
    cursor_entity_merge_threshold: float = 0.95
    
    # Response Storage
    cursor_response_storage_path: Path = Path("responses")
    cursor_response_summary_threshold: int = 1000  # chars
    cursor_response_summary_max_length: int = 500
    cursor_code_block_separate: bool = True
    
    # Git Integration
    cursor_git_enabled: bool = True
    cursor_git_link_window_minutes: int = 10
    cursor_git_track_extensions: list[str] = [".py", ".ts", ".tsx", ".md"]
    
    # Performance
    cursor_max_context_tokens: int = 4000
    cursor_cache_embeddings: bool = True
    cursor_cache_ttl_seconds: int = 3600
    
    # Monitoring
    cursor_enable_metrics: bool = True
    cursor_log_level: str = "INFO"
    cursor_worker_health_check_interval: int = 30
```

---

## Monitoring & Observability

### Key Metrics to Track

```python
# Hot Path Metrics
cursor_hot_path_duration_ms = Histogram(
    "cursor_hot_path_duration_milliseconds",
    "Time for hot path processing",
    buckets=[10, 25, 50, 75, 100, 150, 200, 300]
)

cursor_entity_matches_count = Histogram(
    "cursor_entity_matches",
    "Number of entities matched per query",
    buckets=[0, 1, 2, 5, 10, 20, 50]
)

cursor_context_retrieval_time_ms = Histogram(
    "cursor_context_retrieval_milliseconds",
    "Time to build context",
    buckets=[10, 20, 30, 50, 80, 120, 200]
)

# Worker Metrics
cursor_worker_processing_time = Histogram(
    "cursor_worker_processing_seconds",
    "Worker processing time",
    labelnames=["worker_name"],
    buckets=[0.1, 0.2, 0.5, 1.0, 2.0, 5.0, 10.0]
)

cursor_worker_queue_size = Gauge(
    "cursor_worker_queue_size",
    "Pending items in worker queue",
    labelnames=["worker_name"]
)

cursor_worker_errors = Counter(
    "cursor_worker_errors_total",
    "Worker errors",
    labelnames=["worker_name", "error_type"]
)

# Entity Metrics
cursor_entities_total = Gauge(
    "cursor_entities_total",
    "Total entities in graph",
    labelnames=["entity_type"]
)

cursor_entity_merges_total = Counter(
    "cursor_entity_merges_total",
    "Total entity merges"
)

# Similarity Metrics
cursor_similarity_score = Histogram(
    "cursor_similarity_score",
    "Distribution of similarity scores",
    buckets=[0.5, 0.6, 0.7, 0.75, 0.8, 0.85, 0.9, 0.95, 1.0]
)

# Response Storage Metrics
cursor_responses_summarized = Counter(
    "cursor_responses_summarized_total",
    "Total responses summarized"
)

cursor_response_file_size_bytes = Histogram(
    "cursor_response_file_size_bytes",
    "Response file sizes",
    buckets=[100, 500, 1000, 2000, 5000, 10000, 50000]
)
```

### Health Check Endpoint

```python
@router.get("/api/cursor/health")
async def cursor_health_check():
    """Complete health check for Cursor Memory System."""
    
    health = {
        "status": "healthy",
        "timestamp": datetime.now().isoformat(),
        "components": {}
    }
    
    # Check FalkorDB
    try:
        await repo.query("MATCH (n) RETURN count(n) LIMIT 1")
        health["components"]["falkordb"] = "healthy"
    except Exception as e:
        health["components"]["falkordb"] = f"unhealthy: {e}"
        health["status"] = "degraded"
    
    # Check OpenAI
    try:
        await openai_client.models.list()
        health["components"]["openai"] = "healthy"
    except Exception as e:
        health["components"]["openai"] = f"unhealthy: {e}"
        health["status"] = "degraded"
    
    # Check Workers
    for worker_name in WORKERS:
        status = await check_worker_health(worker_name)
        health["components"][f"worker_{worker_name}"] = status
        if status != "healthy":
            health["status"] = "degraded"
    
    # Check Storage
    if not RESPONSE_STORAGE_PATH.exists():
        health["components"]["storage"] = "unhealthy: path not found"
        health["status"] = "degraded"
    else:
        health["components"]["storage"] = "healthy"
    
    return health
```

---

## Troubleshooting Guide

### Issue: Hot Path Too Slow (>200ms)

**Diagnosis:**
```bash
# Check query timings in logs
grep "Hot path:" /var/log/cursor/app.log | tail -n 100

# Profile specific query
curl -X POST http://localhost:8000/api/cursor/debug/profile \
  -d '{"query": "test query"}'
```

**Solutions:**
- Add index on `Entity.canonical_name`
- Reduce `cursor_context_related_limit`
- Disable code context temporary
- Check FalkorDB memory/CPU

### Issue: Worker Falling Behind

**Diagnosis:**
```bash
# Check worker queue sizes
curl http://localhost:8000/api/cursor/workers/status

# Check logs for errors
grep "Worker error" /var/log/cursor/workers.log
```

**Solutions:**
- Increase worker `batch_size`
- Scale horizontally (multiple workers)
- Increase frequency interval (process less often)
- Check OpenAI rate limits

### Issue: Entities Not Matching

**Diagnosis:**
```bash
# Check entity extraction
curl http://localhost:8000/api/cursor/debug/extract-entities \
  -d '{"text": "Docker and Kubernetes"}'

# Check existing entities
curl http://localhost:8000/api/cursor/entities?type=TECH
```

**Solutions:**
- Check normalization logic
- Add more aliases manually
- Reduce fuzzy_threshold
- Re-run entity extraction on base knowledge

### Issue: Memory Usage High

**Diagnosis:**
```bash
# Check graph size
curl http://localhost:8000/api/cursor/stats

# Check in-memory embeddings
ps aux | grep cursor_workers
```

**Solutions:**
- Move to Redis VSS Ğ´Ğ»Ñ similarity search
- Implement embedding pagination
- Archive old responses
- Increase swap memory

---

## Future Enhancements

### Phase 4+: Advanced Features

**1. Multi-User Knowledge Sharing**
- Cross-user entity graph
- Team knowledge base
- Privacy controls

**2. Semantic Code Search**
- "Find functions similar to this logic"
- Natural language code queries
- Cross-project patterns

**3. Predictive Context**
- ML model Ğ¿Ñ€ĞµĞ´ÑĞºĞ°Ğ·ÑƒÑ” ÑĞºĞ¸Ğ¹ ĞºĞ¾Ğ½Ñ‚ĞµĞºÑÑ‚ Ğ¿Ğ¾Ñ‚Ñ€Ñ–Ğ±ĞµĞ½
- Preload context before query

**4. Time-Travel Queries**
- "How did we implement X in July?"
- Temporal graph visualization

**5. Automatic Documentation Generation**
- Generate docs Ğ· queries + responses
- Keep docs synced Ğ· code changes

**6. Conversation Analytics**
- Most discussed topics
- Knowledge gaps detection
- Query patterns analysis

---

## References & Credits

**Inspiration:**
- [Graphiti](https://github.com/getzep/graphiti) - Temporal knowledge graphs
- [LangChain Memory](https://python.langchain.com/docs/modules/memory/) - Conversation memory patterns
- [Redis VSS](https://redis.io/docs/stack/search/reference/vectors/) - Vector similarity search
- [FalkorDB](https://www.falkordb.com/) - Graph database

**Technologies:**
- OpenAI Embeddings (text-embedding-3-small)
- OpenAI Structured Output (gpt-4o-mini)
- FalkorDB (graph database)
- LangGraph (workflow orchestration)
- NumPy/scikit-learn (similarity calculations)

---

**Version:** 3.0.0  
**Created:** November 13, 2025  
**Status:** ğŸ—ï¸ Design Complete - Ready for Implementation  
**Next Steps:** Begin Phase 1 implementation  
**Estimated Timeline:** 10 weeks to full production deployment
