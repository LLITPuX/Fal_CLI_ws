---
description: Subconscious Agent (–ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å) patterns - Phase 2 Architecture
alwaysApply: false
---
# SUBCONSCIOUS AGENT (–ü–Ü–î–°–í–Ü–î–û–ú–Ü–°–¢–¨) RULES

**Status:** üèóÔ∏è **IN DESIGN** - –∞—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞—Ç–≤–µ—Ä–¥–∂–µ–Ω–∞, –≥–æ—Ç–æ–≤–∞ –¥–æ —ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—ó

---

## Purpose

–ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å - –¥—Ä—É–≥–∏–π –∞–≥–µ–Ω—Ç –≤ pipeline, —è–∫–∏–π –∞–Ω–∞–ª—ñ–∑—É—î –∫–æ–∂–Ω–µ –ø–æ–≤—ñ–¥–æ–º–ª–µ–Ω–Ω—è, —Ä–æ–∑–∫–ª–∞–¥–∞—î –Ω–∞ —Å–µ–º–∞–Ω—Ç–∏—á–Ω—ñ chunks —Ç–∞ entities, –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –∑–≤'—è–∑–∫–∏ –≤ temporal knowledge graph —Ç–∞ —Ñ–æ—Ä–º—É—î –±–∞–≥–∞—Ç–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è Orchestrator.

**–ú–µ—Ç–∞—Ñ–æ—Ä–∞:** –ë–µ–∑–º–µ–∂–Ω–∞ –∫–æ–ª–µ–∫—Ç–∏–≤–Ω–∞ –ø–∞–º'—è—Ç—å –∑ temporal awareness, —è–∫–∞ –∑–Ω–∞—Ö–æ–¥–∏—Ç—å –ø–∞—Ç—Ç–µ—Ä–Ω–∏ —Ç–∞ –∑–≤'—è–∑–∫–∏ –º—ñ–∂ –ø–æ–¥—ñ—è–º–∏ –Ω–µ–∑–∞–ª–µ–∂–Ω–æ –≤—ñ–¥ —á–∞—Å—É —ó—Ö –≤–∏–Ω–∏–∫–Ω–µ–Ω–Ω—è.

---

## Responsibility (Detailed Pipeline)

### ‚úÖ –¢–Ü–õ–¨–ö–ò –¶–ï (5 –∫—Ä–æ–∫—ñ–≤):

1. **TEXT PROCESSOR**: –†–æ–∑–∫–ª–∞—Å—Ç–∏ message –Ω–∞ semantic chunks (–Ω–µ fixed-size!)
2. **EMBEDDINGS GENERATOR**: –°—Ç–≤–æ—Ä–∏—Ç–∏ vector embeddings —á–µ—Ä–µ–∑ OpenAI (batch)
3. **ENTITY EXTRACTOR**: –í–∏—Ç—è–≥—Ç–∏ entities —á–µ—Ä–µ–∑ OpenAI (–æ–∫—Ä–µ–º–∏–π node)
4. **SIMILARITY SEARCH**: –ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ chunks/entities –≤ temporal graph
5. **CONTEXT BUILDER**: –°—Ñ–æ—Ä–º—É–≤–∞—Ç–∏ –±–∞–≥–∞—Ç–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è Orchestrator

### ‚ùå –ó–ê–ë–û–†–û–ù–ï–ù–û:

- ‚ùå –ó–∞–ø–∏—Å—É–≤–∞—Ç–∏ –Ω–æ–≤—ñ messages (—Ü–µ Clerk)
- ‚ùå –ü—Ä–∏–π–º–∞—Ç–∏ —Ä—ñ—à–µ–Ω–Ω—è —â–æ —Ä–æ–±–∏—Ç–∏ –¥–∞–ª—ñ (—Ü–µ Orchestrator)
- ‚ùå –ì–µ–Ω–µ—Ä—É–≤–∞—Ç–∏ –≤—ñ–¥–ø–æ–≤—ñ–¥—ñ –∫–æ—Ä–∏—Å—Ç—É–≤–∞—á—É
- ‚ùå –í–∏–∫–ª–∏–∫–∞—Ç–∏ Gemini –¥–ª—è generation (—Ç—ñ–ª—å–∫–∏ OpenAI –¥–ª—è embeddings/entities)

---

## Module Structure

```
backend/app/agents/subconscious/
‚îú‚îÄ‚îÄ __init__.py              # Public API
‚îú‚îÄ‚îÄ nodes.py                 # subconscious_analyze_node (main LangGraph node)
‚îú‚îÄ‚îÄ text_processor.py        # Semantic chunking logic
‚îú‚îÄ‚îÄ embeddings_service.py    # OpenAI embeddings (batch)
‚îú‚îÄ‚îÄ entity_extractor.py      # OpenAI entity extraction
‚îú‚îÄ‚îÄ similarity_searcher.py   # Cosine similarity search
‚îú‚îÄ‚îÄ graph_builder.py         # Create Chunk/Entity nodes + relationships
‚îú‚îÄ‚îÄ context_formatter.py     # Format context for Orchestrator
‚îú‚îÄ‚îÄ repository.py            # FalkorDB CRUD for chunks/entities
‚îî‚îÄ‚îÄ schemas.py               # Chunk, Entity, ContextAnalysis models
```

---

## Key Architecture Principles

### 1. NO SESSIONS - Temporal Graph

**–ó–∞–º—ñ—Å—Ç—å sessions –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ temporal timestamps:**

```python
# ‚ùå WRONG - Session-based memory
MATCH (m:Message)-[:IN_SESSION]->(s:ChatSession {id: $session_id})
WHERE m.timestamp > $recent_threshold
RETURN m

# ‚úÖ CORRECT - Temporal graph (like Graphiti)
MATCH (m:Message)
WHERE m.timestamp > $time_window_start
  AND m.valid_at <= $current_time
  AND (m.invalid_at IS NULL OR m.invalid_at > $current_time)
RETURN m
ORDER BY m.timestamp DESC
```

**–ü—Ä–∏—á–∏–Ω–∞:** –ë–µ–∑–º–µ–∂–Ω–∞ –ø–∞–º'—è—Ç—å. –ö–æ—Ä–∏—Å—Ç—É–≤–∞—á –º–æ–∂–µ –∑–≥–∞–¥–∞—Ç–∏ —â–æ—Å—å –∑ —Ä–æ–∑–º–æ–≤–∏ 3 –º—ñ—Å—è—Ü—ñ —Ç–æ–º—É, –Ω–µ –∑–Ω–∞—é—á–∏ session_id.

### 2. Semantic Chunking (Not Fixed-Size)

**–†–æ–∑–±–∏–≤–∞—Ç–∏ —Ç–µ–∫—Å—Ç –ø–æ —Å–º–∏—Å–ª–æ–≤–∏—Ö –≥—Ä–∞–Ω–∏—Ü—è—Ö:**

- –ü–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ–∞—Ö (`\n\n`)
- –ü–æ —Ä–µ—á–µ–Ω–Ω—è—Ö (–∑–∞–≤–µ—Ä—à–µ–Ω—ñ –¥—É–º–∫–∏)
- –ó —É—Ä–∞—Ö—É–≤–∞–Ω–Ω—è–º markdown —Å—Ç—Ä—É–∫—Ç—É—Ä–∏ (headings, code blocks)
- Dynamic overlap based on content

**–ü—Ä–∏–∫–ª–∞–¥:**
```
User paste documentation (5000 chars):
"# Installation\n\nTo install...\n\n## Prerequisites\n\nYou need..."

‚Üì Semantic chunking

Chunk 1: "# Installation\nTo install..." (—Ü—ñ–ª–∏–π —Ä–æ–∑–¥—ñ–ª)
Chunk 2: "## Prerequisites\nYou need..." (–ø—ñ–¥—Ä–æ–∑–¥—ñ–ª)
Chunk 3: "## Configuration\nEdit..." (–Ω–∞—Å—Ç—É–ø–Ω–∏–π –ø—ñ–¥—Ä–æ–∑–¥—ñ–ª)
```

### 3. Orchestrator Waits (Synchronous Mode)

```
Clerk (15ms) ‚Üí Subconscious (400ms) ‚Üí Orchestrator (2000ms)
                    ‚Üë
              BLOCKS HERE
          Orchestrator –æ—Ç—Ä–∏–º—É—î
          –ü–û–í–ù–ò–ô –∫–æ–Ω—Ç–µ–∫—Å—Ç
```

**–ù–ï event-driven**, –∞ sequential pipeline.

### 4. Entity Extraction Included

Entities —î **first-class citizens** –≤ –≥—Ä–∞—Ñ—ñ –ø–æ—Ä—è–¥ –∑ chunks.

---

## Temporal Graph Schema

### Node Types

```cypher
// –Ü—Å–Ω—É—é—á–∏–π –∑ Phase 1
(:Message {
    id: "uuid",
    content: "–ø–æ–≤–Ω–∏–π —Ç–µ–∫—Å—Ç",
    role: "user|assistant|system",
    timestamp: "ISO8601",
    
    // Temporal fields (NEW - Graphiti pattern)
    valid_at: "ISO8601",        // –ö–æ–ª–∏ facts —Å—Ç–∞–ª–∏ –∞–∫—Ç—É–∞–ª—å–Ω–∏–º–∏
    invalid_at: "ISO8601|NULL", // –ö–æ–ª–∏ –∑–∞—Å—Ç–∞—Ä—ñ–ª–∏ (NULL = –¥–æ—Å—ñ –∞–∫—Ç—É–∞–ª—å–Ω—ñ)
    
    // Processing metadata
    language: "uk|en|ru|...",
    content_length: number,
    is_processed: boolean       // –ß–∏ –æ–±—Ä–æ–±–ª–µ–Ω–æ –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—é
})

// –ù–û–í–ò–ô: Semantic Chunk
(:Chunk {
    id: "uuid",
    content: "—á–∞—Å—Ç–∏–Ω–∞ —Ç–µ–∫—Å—Ç—É",
    position: number,           // –ü–æ—Ä—è–¥–∫–æ–≤–∏–π –Ω–æ–º–µ—Ä –≤ message
    char_start: number,
    char_end: number,
    chunk_type: "paragraph|sentence|code|heading",
    
    // Temporal
    created_at: "ISO8601",
    valid_at: "ISO8601",
    invalid_at: "ISO8601|NULL",
    
    // Embeddings
    embedding: [float, ...],    // 1536-dim vector (OpenAI text-embedding-3-small)
    embedding_model: "text-embedding-3-small",
    embedding_created_at: "ISO8601"
})

// –ù–û–í–ò–ô: Entity (Named Entities)
(:Entity {
    id: "uuid",
    name: "original name",      // –û—Ä–∏–≥—ñ–Ω–∞–ª—å–Ω–∞ —Ñ–æ—Ä–º–∞
    canonical_name: "normalized", // –ù–æ—Ä–º–∞–ª—ñ–∑–æ–≤–∞–Ω–∞ (lowercase, trimmed)
    type: "PERSON|ORG|LOCATION|TECH|CONCEPT|...",
    
    // Temporal
    first_seen: "ISO8601",      // –ö–æ–ª–∏ –≤–ø–µ—Ä—à–µ –∑–≥–∞–¥–∞–ª–∏
    last_seen: "ISO8601",       // –û—Å—Ç–∞–Ω–Ω—î –∑–≥–∞–¥—É–≤–∞–Ω–Ω—è
    valid_at: "ISO8601",
    invalid_at: "ISO8601|NULL",
    
    // Embeddings
    embedding: [float, ...],
    embedding_model: "text-embedding-3-small",
    
    // Metadata
    mention_count: number,      // –°–∫—ñ–ª—å–∫–∏ —Ä–∞–∑—ñ–≤ –∑–≥–∞–¥—É–≤–∞–ª–∏
    confidence: float           // –í–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å extraction (0.0-1.0)
})
```

### Relationship Types

```cypher
// Chunk ‚Üí Message
(:Chunk)-[:PART_OF {
    position: number,
    created_at: "ISO8601"
}]->(:Message)

// Chunk ‚Üî Chunk (Semantic Similarity)
(:Chunk)-[:SIMILAR_TO {
    similarity: float,          // Cosine similarity (0.0-1.0)
    created_at: "ISO8601",
    algorithm: "cosine"
}]-(:Chunk)

// Chunk ‚Üí Entity
(:Chunk)-[:MENTIONS {
    position: number,           // –ü–æ–∑–∏—Ü—ñ—è –≤ chunk
    context: "surrounding text",
    confidence: float
}]->(:Entity)

// Message ‚Üí Entity (Aggregated)
(:Message)-[:DISCUSSES {
    mention_count: number,
    salience: float,            // –í–∞–∂–ª–∏–≤—ñ—Å—Ç—å entity –≤ message
    created_at: "ISO8601"
}]->(:Entity)

// Entity ‚Üî Entity (Co-occurrence)
(:Entity)-[:RELATED_TO {
    co_occurrence_count: number,
    strength: float,            // –°–∏–ª–∞ –∑–≤'—è–∑–∫—É
    contexts: ["msg_id1", "msg_id2"]
}]-(:Entity)

// Message ‚Üî Message (—á–µ—Ä–µ–∑ —Å—Ö–æ–∂—ñ chunks)
(:Message)-[:SEMANTICALLY_RELATED {
    similarity: float,
    via_chunks: ["chunk1", "chunk2"],
    created_at: "ISO8601"
}]-(:Message)

// Temporal sequence (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ)
(:Message)-[:FOLLOWED_BY {
    time_delta_seconds: number
}]->(:Message)
```

---

## Technologies Stack

### OpenAI Models

```python
# 1. Embeddings
model: "text-embedding-3-small"
dimensions: 1536
cost: ~$0.00002 per 1K tokens
max_input: 8,191 tokens

# 2. Entity Extraction
model: "gpt-4o-mini"  # –®–≤–∏–¥–∫–∏–π —ñ –¥–µ—à–µ–≤–∏–π –¥–ª—è NER
cost: ~$0.15 per 1M input tokens
structured_output: JSON mode
```

### Vector Storage Strategy

**Option A: FalkorDB native (—è–∫—â–æ –ø—ñ–¥—Ç—Ä–∏–º—É—î vector ops)**
```cypher
// Store as array property
CREATE (c:Chunk {embedding: [0.1, 0.2, ..., 0.9]})

// Search (—è–∫—â–æ —î vector functions)
MATCH (c:Chunk)
WITH c, vector.cosine(c.embedding, $query_embedding) AS similarity
WHERE similarity > 0.7
RETURN c
ORDER BY similarity DESC
LIMIT 10
```

**Option B: In-Memory (—è–∫—â–æ FalkorDB –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î)**
```python
# –í–∏—Ç—è–≥–Ω—É—Ç–∏ –≤—Å—ñ embeddings
chunks = await repo.get_all_chunks_with_embeddings()

# NumPy/scikit-learn cosine similarity
from sklearn.metrics.pairwise import cosine_similarity
similarities = cosine_similarity([query_emb], [c.embedding for c in chunks])

# Top-K
top_indices = similarities[0].argsort()[-10:][::-1]
```

**Option C: Redis Stack VSS Module (—è–∫—â–æ —Ç—Ä–µ–±–∞ scale)**
```bash
# RediSearch with vector similarity
FT.SEARCH idx:chunks 
  "*=>[KNN 10 @embedding $vec AS score]" 
  PARAMS 2 vec <binary>
  RETURN 3 id content score
```

**–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü—ñ—è:** –ü–æ—á–∞—Ç–∏ –∑ **Option B** (–ø—Ä–∞—Ü—é—î –∑–∞–≤–∂–¥–∏), –ø–æ—Ç—ñ–º –ø–µ—Ä–µ–≤—ñ—Ä–∏—Ç–∏ **Option A**, —è–∫—â–æ —Ç—Ä–µ–±–∞ —à–≤–∏–¥—à–µ - **Option C**.

---

## Semantic Chunking Strategy

### Algorithm (LangChain-inspired)

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

class SemanticTextSplitter:
    """Split text –ø–æ —Å–º–∏—Å–ª–æ–≤–∏—Ö –≥—Ä–∞–Ω–∏—Ü—è—Ö."""
    
    def __init__(self):
        self.separators = [
            "\n\n",      # –ü–∞—Ä–∞–≥—Ä–∞—Ñ–∏ (–Ω–∞–π–≤–∏—â–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
            "\n",        # –†—è–¥–∫–∏
            ". ",        # –†–µ—á–µ–Ω–Ω—è
            "! ",
            "? ",
            "; ",
            ": ",
            ", ",
            " ",         # –°–ª–æ–≤–∞ (–Ω–∞–π–Ω–∏–∂—á–∏–π –ø—Ä—ñ–æ—Ä–∏—Ç–µ—Ç)
        ]
    
    async def split(self, text: str, max_chunk_size: int = 800) -> list[Chunk]:
        """
        –†–æ–∑–±–∏—Ç–∏ —Ç–µ–∫—Å—Ç –Ω–∞ semantic chunks.
        
        Logic:
        1. –°–ø—Ä–æ–±—É–≤–∞—Ç–∏ —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –ø–æ \n\n (–ø–∞—Ä–∞–≥—Ä–∞—Ñ–∏)
        2. –Ø–∫—â–æ –ø–∞—Ä–∞–≥—Ä–∞—Ñ > max_chunk_size, —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –ø–æ —Ä–µ—á–µ–Ω–Ω—è—Ö
        3. –Ø–∫—â–æ —Ä–µ—á–µ–Ω–Ω—è > max_chunk_size, —Ä–æ–∑–¥—ñ–ª–∏—Ç–∏ –ø–æ —Å–ª–æ–≤–∞—Ö
        4. –î–æ–¥–∞—Ç–∏ overlap –º—ñ–∂ chunks (10-15%)
        """
        splitter = RecursiveCharacterTextSplitter(
            separators=self.separators,
            chunk_size=max_chunk_size,
            chunk_overlap=int(max_chunk_size * 0.15),  # 15% overlap
            length_function=len,
            is_separator_regex=False,
        )
        
        texts = splitter.split_text(text)
        
        chunks = []
        char_position = 0
        for i, text in enumerate(texts):
            chunk_type = self._detect_chunk_type(text)
            chunks.append(Chunk(
                content=text,
                position=i,
                char_start=char_position,
                char_end=char_position + len(text),
                chunk_type=chunk_type,
            ))
            char_position += len(text)
        
        return chunks
    
    def _detect_chunk_type(self, text: str) -> str:
        """–í–∏–∑–Ω–∞—á–∏—Ç–∏ —Ç–∏–ø chunk."""
        if text.startswith("#"):
            return "heading"
        elif text.startswith("```"):
            return "code"
        elif "\n" not in text and len(text) < 200:
            return "sentence"
        else:
            return "paragraph"
```

### Chunk Size Guidelines

| Content Type | Chunk Size | Overlap | Reason |
|--------------|------------|---------|---------|
| Chat messages | 500-800 | 10% | –ö–æ—Ä–æ—Ç–∫—ñ, —Ü—ñ–ª—ñ—Å–Ω—ñ –¥—É–º–∫–∏ |
| Documentation | 800-1200 | 15% | Structured, headings |
| Code/logs | 600-1000 | 20% | Context –≤–∞–∂–ª–∏–≤–∏–π |
| Long articles | 1000-1500 | 15% | –ü–∞—Ä–∞–≥—Ä–∞—Ñ–∏ |

---

## Entity Extraction

### OpenAI Structured Output

```python
from openai import AsyncOpenAI
from pydantic import BaseModel

class ExtractedEntity(BaseModel):
    """Single entity."""
    name: str
    type: str  # PERSON, ORG, LOCATION, TECH, CONCEPT, etc.
    confidence: float
    context: str  # Surrounding text

class EntityExtractionResult(BaseModel):
    """–†–µ–∑—É–ª—å—Ç–∞—Ç extraction."""
    entities: list[ExtractedEntity]

class EntityExtractor:
    """Extract entities —á–µ—Ä–µ–∑ OpenAI."""
    
    def __init__(self, api_key: str):
        self.client = AsyncOpenAI(api_key=api_key)
        self.model = "gpt-4o-mini"
    
    async def extract(self, text: str) -> list[ExtractedEntity]:
        """
        Extract named entities –∑ —Ç–µ–∫—Å—Ç—É.
        
        Uses JSON mode –¥–ª—è structured output.
        """
        prompt = f"""Extract all named entities from the following text.
        
Categories:
- PERSON: People, characters
- ORG: Organizations, companies, teams
- LOCATION: Countries, cities, places
- TECH: Technologies, programming languages, frameworks
- CONCEPT: Abstract concepts, methodologies
- EVENT: Events, conferences, meetings

Text:
{text}

Return entities with:
- name: exact text from input
- type: category from above
- confidence: 0.0-1.0 (how confident you are)
- context: surrounding words for disambiguation
"""
        
        response = await self.client.chat.completions.create(
            model=self.model,
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"},
            temperature=0.0,  # –î–µ—Ç–µ—Ä–º—ñ–Ω—ñ—Å—Ç–∏—á–Ω–∏–π output
        )
        
        result = EntityExtractionResult.model_validate_json(
            response.choices[0].message.content
        )
        
        return result.entities
```

### Entity Normalization

```python
def normalize_entity(name: str, type: str) -> str:
    """
    –ù–æ—Ä–º–∞–ª—ñ–∑—É–≤–∞—Ç–∏ entity name –¥–ª—è –≥—Ä—É–ø—É–≤–∞–Ω–Ω—è.
    
    Examples:
    - "Docker" vs "docker" ‚Üí "docker"
    - "Kubernetes" vs "K8s" ‚Üí "kubernetes" (–º–æ–∂–Ω–∞ –¥–æ–¥–∞—Ç–∏ aliases)
    - "Python 3.12" vs "Python" ‚Üí "python"
    """
    canonical = name.lower().strip()
    
    # Type-specific normalization
    if type == "TECH":
        # Remove version numbers
        canonical = re.sub(r'\s+\d+(\.\d+)*', '', canonical)
        # Known aliases
        aliases = {
            "k8s": "kubernetes",
            "js": "javascript",
            "ts": "typescript",
        }
        canonical = aliases.get(canonical, canonical)
    
    return canonical
```

---

## Similarity Search Algorithm

### Step-by-Step

```python
class SimilaritySearcher:
    """Find similar chunks/entities –≤ temporal graph."""
    
    def __init__(self, repository: SubconsciousRepository):
        self.repo = repository
        self.threshold = 0.7  # Minimum similarity
    
    async def find_similar_chunks(
        self,
        query_embedding: list[float],
        top_k: int = 10,
        time_window_days: int | None = None,  # Temporal filter
    ) -> list[SimilarChunk]:
        """
        –ó–Ω–∞–π—Ç–∏ –Ω–∞–π–±—ñ–ª—å—à —Å—Ö–æ–∂—ñ chunks.
        
        Args:
            query_embedding: Vector –¥–ª—è –Ω–æ–≤–æ–≥–æ chunk
            top_k: –°–∫—ñ–ª—å–∫–∏ –ø–æ–≤–µ—Ä–Ω—É—Ç–∏
            time_window_days: –Ø–∫—â–æ None - –≤–µ—Å—å —á–∞—Å, —ñ–Ω–∞–∫—à–µ –æ—Å—Ç–∞–Ω–Ω—ñ N –¥–Ω—ñ–≤
        """
        # 1. Get all chunks (–∑ temporal filter)
        if time_window_days:
            cutoff = datetime.now() - timedelta(days=time_window_days)
            chunks = await self.repo.get_chunks_since(cutoff)
        else:
            chunks = await self.repo.get_all_chunks_with_embeddings()
        
        if not chunks:
            return []
        
        # 2. Calculate cosine similarity (NumPy)
        from sklearn.metrics.pairwise import cosine_similarity
        
        chunk_embeddings = np.array([c.embedding for c in chunks])
        query_emb = np.array([query_embedding])
        
        similarities = cosine_similarity(query_emb, chunk_embeddings)[0]
        
        # 3. Filter by threshold
        mask = similarities > self.threshold
        filtered_indices = np.where(mask)[0]
        filtered_similarities = similarities[mask]
        
        # 4. Sort and get top-K
        top_indices = filtered_similarities.argsort()[-top_k:][::-1]
        
        results = []
        for idx in top_indices:
            chunk_idx = filtered_indices[idx]
            results.append(SimilarChunk(
                chunk=chunks[chunk_idx],
                similarity=float(filtered_similarities[idx]),
            ))
        
        return results
    
    async def find_similar_entities(
        self,
        entity_name: str,
        entity_type: str,
    ) -> list[Entity]:
        """
        –ó–Ω–∞–π—Ç–∏ —Å—Ö–æ–∂—ñ entities (–ø–æ canonical_name + type).
        """
        canonical = normalize_entity(entity_name, entity_type)
        
        # Cypher query
        cypher = """
        MATCH (e:Entity)
        WHERE e.type = $type
          AND (e.canonical_name = $canonical 
               OR e.name =~ $fuzzy_pattern)
        RETURN e
        ORDER BY e.mention_count DESC
        LIMIT 10
        """
        
        # Fuzzy matching pattern
        fuzzy = f"(?i).*{re.escape(canonical)}.*"
        
        results = await self.repo.query(cypher, {
            "type": entity_type,
            "canonical": canonical,
            "fuzzy_pattern": fuzzy,
        })
        
        return [Entity(**r["e"]) for r in results]
```

---

## Context Builder

### Context Structure –¥–ª—è Orchestrator

```python
class ContextAnalysis(BaseModel):
    """–ö–æ–Ω—Ç–µ–∫—Å—Ç —â–æ –ø–µ—Ä–µ–¥–∞—î—Ç—å—Å—è Orchestrator."""
    
    # 1. Recent temporal context
    recent_messages: list[Message] = []  # –û—Å—Ç–∞–Ω–Ω—ñ 10 messages (–ø–æ timestamp)
    
    # 2. Semantic matches
    similar_chunks: list[SimilarChunk] = []  # Top-10 —Å—Ö–æ–∂–∏—Ö chunks
    similar_messages: list[Message] = []     # Messages —â–æ –º—ñ—Å—Ç—è—Ç—å —Ü—ñ chunks
    
    # 3. Entities
    mentioned_entities: list[Entity] = []    # Entities –∑ current message
    related_entities: list[Entity] = []      # –°—Ö–æ–∂—ñ entities –∑ —ñ—Å—Ç–æ—Ä—ñ—ó
    entity_context: dict[str, list[str]] = {} # Entity -> contexts –¥–µ –∑–≥–∞–¥—É—î—Ç—å—Å—è
    
    # 4. Graph insights
    topics: list[str] = []                   # –í–∏—è–≤–ª–µ–Ω—ñ —Ç–µ–º–∏ (–∑ entities)
    is_new_topic: bool = False               # –ß–∏ –Ω–æ–≤–∏–π —Ç–æ–ø—ñ–∫
    conversation_continuity: float = 0.0     # 0.0-1.0 (–Ω–∞—Å–∫—ñ–ª—å–∫–∏ –ø–æ–≤'—è–∑–∞–Ω–æ –∑ –º–∏–Ω—É–ª–∏–º)
    
    # 5. Temporal insights
    time_span_days: int = 0                  # –î—ñ–∞–ø–∞–∑–æ–Ω —á–∞—Å—É –≤ –∫–æ–Ω—Ç–µ–∫—Å—Ç—ñ
    oldest_relevant_message: datetime | None = None
    
    # 6. Metadata
    total_chunks_analyzed: int = 0
    total_entities_extracted: int = 0
    processing_time_ms: float = 0.0
    confidence: float = 0.0                  # –ó–∞–≥–∞–ª—å–Ω–∞ –≤–ø–µ–≤–Ω–µ–Ω—ñ—Å—Ç—å


class ContextFormatter:
    """–§–æ—Ä–º—É–≤–∞—Ç–∏ –∫–æ–Ω—Ç–µ–∫—Å—Ç –¥–ª—è Orchestrator."""
    
    async def build_context(
        self,
        message: Message,
        chunks: list[Chunk],
        entities: list[Entity],
        similar_chunks: list[SimilarChunk],
    ) -> ContextAnalysis:
        """
        –ü–æ–±—É–¥—É–≤–∞—Ç–∏ –ø–æ–≤–Ω–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç.
        """
        context = ContextAnalysis()
        
        # 1. Recent messages (temporal query)
        context.recent_messages = await self._get_recent_messages(
            reference_time=message.timestamp,
            limit=10
        )
        
        # 2. Semantic matches
        context.similar_chunks = similar_chunks
        context.similar_messages = await self._get_messages_for_chunks(
            [sc.chunk for sc in similar_chunks]
        )
        
        # 3. Entities
        context.mentioned_entities = entities
        context.related_entities = await self._find_related_entities(entities)
        context.entity_context = await self._build_entity_contexts(entities)
        
        # 4. Topics
        context.topics = self._extract_topics(entities)
        context.is_new_topic = self._detect_new_topic(
            context.topics,
            context.recent_messages
        )
        
        # 5. Continuity
        context.conversation_continuity = self._calculate_continuity(
            similar_chunks
        )
        
        # 6. Temporal
        if context.similar_messages:
            oldest = min(m.timestamp for m in context.similar_messages)
            context.oldest_relevant_message = oldest
            context.time_span_days = (
                message.timestamp - oldest
            ).days
        
        # 7. Metadata
        context.total_chunks_analyzed = len(chunks)
        context.total_entities_extracted = len(entities)
        context.confidence = self._calculate_confidence(
            similar_chunks, entities
        )
        
        return context
    
    async def _get_recent_messages(
        self,
        reference_time: datetime,
        limit: int = 10
    ) -> list[Message]:
        """
        –û—Ç—Ä–∏–º–∞—Ç–∏ recent messages –ë–ï–ó session filter (temporal only).
        """
        cypher = """
        MATCH (m:Message)
        WHERE m.timestamp < $reference_time
          AND m.valid_at <= $reference_time
          AND (m.invalid_at IS NULL OR m.invalid_at > $reference_time)
        RETURN m
        ORDER BY m.timestamp DESC
        LIMIT $limit
        """
        # ... implementation
    
    def _extract_topics(self, entities: list[Entity]) -> list[str]:
        """
        –í–∏—Ç—è–≥—Ç–∏ topics –∑ entities.
        
        Strategy:
        - Group entities by type
        - Most mentioned entities = topics
        - TECH entities –æ—Å–æ–±–ª–∏–≤–æ –≤–∞–∂–ª–∏–≤—ñ
        """
        tech_entities = [e for e in entities if e.type == "TECH"]
        concept_entities = [e for e in entities if e.type == "CONCEPT"]
        
        topics = []
        topics.extend([e.canonical_name for e in tech_entities[:5]])
        topics.extend([e.canonical_name for e in concept_entities[:3]])
        
        return topics
    
    def _calculate_continuity(
        self,
        similar_chunks: list[SimilarChunk]
    ) -> float:
        """
        –†–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ conversation continuity.
        
        High continuity = –±–∞–≥–∞—Ç–æ —Å—Ö–æ–∂–∏—Ö chunks –∑ –≤–∏—Å–æ–∫–æ—é similarity
        Low continuity = –Ω—ñ—á–æ–≥–æ —Å—Ö–æ–∂–æ–≥–æ (–Ω–æ–≤–∏–π —Ç–æ–ø—ñ–∫)
        """
        if not similar_chunks:
            return 0.0
        
        # Average similarity –∑ weight –ø–æ top chunks
        weights = [1.0, 0.8, 0.6, 0.4, 0.2]  # Top-5
        weighted_sim = sum(
            sc.similarity * weights[i]
            for i, sc in enumerate(similar_chunks[:5])
        ) / sum(weights[:len(similar_chunks)])
        
        return weighted_sim
```

---

## Node Implementation Pattern

```python
async def subconscious_analyze_node(
    state: dict,
    repository: SubconsciousRepository,
    text_processor: SemanticTextSplitter,
    embeddings_service: EmbeddingsService,
    entity_extractor: EntityExtractor,
    similarity_searcher: SimilaritySearcher,
    context_formatter: ContextFormatter,
) -> dict:
    """
    –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å node - –ø–æ–≤–Ω–∏–π pipeline.
    
    Sequential execution (Orchestrator WAITS):
    1. Get message –≤—ñ–¥ Clerk
    2. Semantic chunking
    3. Generate embeddings (batch)
    4. Extract entities
    5. Find similar chunks/entities
    6. Build context
    7. Update graph
    8. Return enriched state
    """
    logger.info("üß† –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å: –ü–æ—á–∏–Ω–∞—é –∞–Ω–∞–ª—ñ–∑...")
    start_time = time.time()
    
    try:
        # 0. Check if recorded
        if not state.get("recorded"):
            logger.warning("‚ö†Ô∏è Message not recorded, skipping analysis")
            return state
        
        # 1. Get message
        message_id = state["message_id"]
        message = await repository.get_message(message_id)
        
        # 2. Semantic chunking
        logger.info("üìÑ Chunking text...")
        chunks = await text_processor.split(message.content)
        logger.info(f"Created {len(chunks)} semantic chunks")
        
        # 3. Generate embeddings (BATCH)
        logger.info("üî¢ Generating embeddings...")
        chunk_texts = [c.content for c in chunks]
        embeddings = await embeddings_service.generate_batch(chunk_texts)
        
        for chunk, embedding in zip(chunks, embeddings):
            chunk.embedding = embedding
        
        # 4. Extract entities
        logger.info("üè∑Ô∏è Extracting entities...")
        entities = await entity_extractor.extract(message.content)
        logger.info(f"Extracted {len(entities)} entities")
        
        # 5. Find similar chunks
        logger.info("üîç Searching for similar chunks...")
        similar_results = []
        for chunk in chunks:
            similar = await similarity_searcher.find_similar_chunks(
                chunk.embedding,
                top_k=10
            )
            similar_results.extend(similar)
        
        # Deduplicate and sort
        similar_results = sorted(
            {sr.chunk.id: sr for sr in similar_results}.values(),
            key=lambda x: x.similarity,
            reverse=True
        )[:10]
        
        logger.info(f"Found {len(similar_results)} similar chunks")
        
        # 6. Build context
        logger.info("üìã Building context...")
        context = await context_formatter.build_context(
            message=message,
            chunks=chunks,
            entities=entities,
            similar_chunks=similar_results,
        )
        
        # 7. Save to graph
        logger.info("üíæ Saving to graph...")
        await repository.save_chunks(chunks, message_id)
        await repository.save_entities(entities, message_id, chunks)
        await repository.create_similarity_edges(similar_results)
        
        # 8. Update state
        processing_time = (time.time() - start_time) * 1000
        context.processing_time_ms = processing_time
        
        state["context"] = context.model_dump()
        state["analyzed"] = True
        state["error"] = None
        
        logger.info(
            f"üß† –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å –∑–∞–≤–µ—Ä—à–∏–ª–∞ –∞–Ω–∞–ª—ñ–∑: "
            f"{len(chunks)} chunks, {len(entities)} entities, "
            f"similarity={context.conversation_continuity:.2f}, "
            f"time={processing_time:.0f}ms"
        )
        
    except Exception as e:
        logger.error(f"üß† –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å: –ü–æ–º–∏–ª–∫–∞ –∞–Ω–∞–ª—ñ–∑—É: {e}", exc_info=True)
        state["analyzed"] = False
        state["error"] = f"Analysis failed: {str(e)}"
        # Graceful degradation - –ø—É—Å—Ç–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç
        state["context"] = ContextAnalysis().model_dump()
    
    return state
```

---

## Performance Targets

### Expected Timings (per message)

| Operation | Time (avg) | Notes |
|-----------|------------|-------|
| Semantic chunking (1000 chars) | 10-20ms | Pure Python |
| OpenAI embeddings (10 chunks, batch) | 150-250ms | Network I/O |
| Entity extraction (1 request) | 200-400ms | OpenAI API |
| Similarity search (1000 chunks) | 30-80ms | In-memory NumPy |
| Graph writes (chunks + entities + edges) | 50-150ms | FalkorDB batch |
| Context building | 20-50ms | Queries + formatting |
| **TOTAL** | **460-950ms** | ~0.5-1s acceptable |

### Scaling Considerations

**–î–æ 10,000 chunks –≤ –ë–î:**
- ‚úÖ In-memory similarity –ø—Ä–∞—Ü—é—î (~100ms)

**10,000-100,000 chunks:**
- ‚ö†Ô∏è In-memory –ø–æ—á–∏–Ω–∞—î –≥–∞–ª—å–º—É–≤–∞—Ç–∏ (~500ms+)
- üîß –†—ñ—à–µ–Ω–Ω—è: Redis Stack VSS –∞–±–æ Qdrant

**100,000+ chunks:**
- ‚ùå –û–±–æ–≤'—è–∑–∫–æ–≤–æ vector DB
- ‚úÖ Redis Stack VSS: <50ms
- ‚úÖ Qdrant: <30ms

---

## Error Handling Strategy

### Three-Layer Approach

**1. Service Layer ‚Üí Specific Errors**
```python
class EmbeddingError(Exception):
    """OpenAI embedding failed."""

class EntityExtractionError(Exception):
    """Entity extraction failed."""

class SimilaritySearchError(Exception):
    """Similarity search failed."""
```

**2. Node Layer ‚Üí Graceful Degradation**
```python
try:
    context = await build_full_context(...)
except EmbeddingError:
    # Fallback: –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ–∑ semantic search
    context = await build_basic_context(recent_only=True)
except EntityExtractionError:
    # Fallback: –∫–æ–Ω—Ç–µ–∫—Å—Ç –±–µ–∑ entities
    context = await build_context_without_entities(...)
```

**3. API Layer ‚Üí HTTP Responses**
```python
# –Ø–∫—â–æ state["analyzed"] = False
if not final_state.analyzed:
    logger.warning("Subconscious analysis failed, using degraded context")
    # Orchestrator –æ—Ç—Ä–∏–º—É—î –±–∞–∑–æ–≤–∏–π –∫–æ–Ω—Ç–µ–∫—Å—Ç –∞–ª–µ –ø—Ä–∞—Ü—é—î –¥–∞–ª—ñ
```

**–ù–Ü–ö–û–õ–ò –Ω–µ fail—É–≤–∞—Ç–∏ –≤–µ—Å—å request —á–µ—Ä–µ–∑ Subconscious!**

---

## Configuration

```python
# core/config.py

class Settings(BaseSettings):
    # ... existing ...
    
    # OpenAI Settings
    openai_api_key: str = os.getenv("OPENAI_API_KEY")
    openai_embedding_model: str = "text-embedding-3-small"
    openai_embedding_dimensions: int = 1536
    openai_entity_model: str = "gpt-4o-mini"
    
    # Subconscious Settings
    subconscious_chunk_size: int = 800
    subconscious_chunk_overlap: float = 0.15  # 15%
    subconscious_similarity_threshold: float = 0.7
    subconscious_max_similar_chunks: int = 10
    subconscious_recent_messages_limit: int = 10
    
    # Temporal settings
    subconscious_default_time_window_days: int | None = None  # None = –±–µ–∑–º–µ–∂–Ω–æ
    
    # Performance
    subconscious_batch_size: int = 100  # Max chunks per batch
    subconscious_timeout: int = 30  # seconds
```

---

## Testing Strategy

### Unit Tests

```python
@pytest.mark.asyncio
async def test_semantic_chunking():
    """Test semantic text splitting."""
    processor = SemanticTextSplitter()
    
    text = "# Heading\n\nParagraph 1.\n\nParagraph 2.\n\nCode:\n```python\ncode\n```"
    chunks = await processor.split(text)
    
    assert len(chunks) == 3
    assert chunks[0].chunk_type == "heading"
    assert chunks[1].chunk_type == "paragraph"
    assert chunks[2].chunk_type == "code"

@pytest.mark.asyncio
async def test_entity_extraction(mock_openai):
    """Test entity extraction."""
    extractor = EntityExtractor(api_key="test")
    
    text = "Docker —Ç–∞ Kubernetes –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—é—Ç—å –≤ Google."
    entities = await extractor.extract(text)
    
    assert len(entities) >= 3
    assert any(e.name == "Docker" and e.type == "TECH" for e in entities)
    assert any(e.name == "Google" and e.type == "ORG" for e in entities)

@pytest.mark.asyncio
async def test_similarity_search():
    """Test finding similar chunks."""
    searcher = SimilaritySearcher(mock_repository)
    
    query_embedding = [0.1] * 1536
    results = await searcher.find_similar_chunks(query_embedding, top_k=5)
    
    assert len(results) <= 5
    assert all(r.similarity >= 0.7 for r in results)
    assert results[0].similarity >= results[-1].similarity  # Sorted
```

### Integration Tests

```python
@pytest.mark.asyncio
async def test_full_subconscious_pipeline(test_db):
    """Test –ø–æ–≤–Ω–∏–π pipeline –ü—ñ–¥—Å–≤—ñ–¥–æ–º–æ—Å—Ç—ñ."""
    # Setup
    message = await create_test_message("–†–æ–∑–∫–∞–∂–∏ –ø—Ä–æ Docker —Ç–∞ Kubernetes")
    
    state = {
        "message_id": message.id,
        "recorded": True,
    }
    
    # Execute
    result = await subconscious_analyze_node(state, ...)
    
    # Assert
    assert result["analyzed"] is True
    assert "context" in result
    
    context = ContextAnalysis(**result["context"])
    assert len(context.similar_chunks) > 0
    assert len(context.mentioned_entities) >= 2
    assert "docker" in [e.canonical_name for e in context.mentioned_entities]
```

---

## Monitoring & Observability

### Key Metrics

```python
# Prometheus metrics (–ø—Ä–∏–∫–ª–∞–¥)
subconscious_processing_time = Histogram(
    "subconscious_processing_seconds",
    "Time to process message through subconscious",
    buckets=[0.1, 0.25, 0.5, 0.75, 1.0, 2.0]
)

subconscious_chunks_created = Counter(
    "subconscious_chunks_total",
    "Total chunks created"
)

subconscious_entities_extracted = Counter(
    "subconscious_entities_total",
    "Total entities extracted"
)

subconscious_similarity_score = Histogram(
    "subconscious_similarity_score",
    "Distribution of similarity scores",
    buckets=[0.5, 0.6, 0.7, 0.8, 0.9, 0.95, 1.0]
)
```

### Logging Strategy

```python
logger.info(
    f"üß† –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å: Processed message "
    f"(chunks={len(chunks)}, "
    f"entities={len(entities)}, "
    f"similarity_avg={avg_similarity:.2f}, "
    f"continuity={context.conversation_continuity:.2f}, "
    f"time={processing_time:.0f}ms)"
)
```

---

## Integration with LangGraph

### Workflow Update

```python
# backend/app/agents/graph.py

def create_chat_workflow(
    clerk_repo: MessageRepository,
    subconscious_repo: SubconsciousRepository,
    # ... all subconscious dependencies
) -> StateGraph:
    """
    Create multi-agent workflow with Subconscious.
    
    Flow:
    Entry ‚Üí Clerk ‚Üí Subconscious ‚Üí Orchestrator ‚Üí END
    """
    workflow = StateGraph(ChatState)
    
    # Clerk node
    async def clerk_node_wrapper(state: ChatState) -> ChatState:
        # ... existing ...
    
    # Subconscious node (NEW)
    async def subconscious_node_wrapper(state: ChatState) -> ChatState:
        """Wrapper –∑ dependency injection."""
        state_dict = state.model_dump()
        
        # Check if should analyze
        if not state_dict.get("recorded"):
            return state
        
        # Run analysis
        updated_state = await subconscious_analyze_node(
            state_dict,
            repository=subconscious_repo,
            text_processor=text_processor,
            embeddings_service=embeddings_service,
            entity_extractor=entity_extractor,
            similarity_searcher=similarity_searcher,
            context_formatter=context_formatter,
        )
        
        return ChatState(**updated_state)
    
    # Add nodes
    workflow.add_node("clerk", clerk_node_wrapper)
    workflow.add_node("subconscious", subconscious_node_wrapper)
    
    # Define flow
    workflow.set_entry_point("clerk")
    workflow.add_edge("clerk", "subconscious")
    workflow.add_edge("subconscious", END)  # Orchestrator –≤ Phase 3
    
    return workflow.compile()
```

---

## State Contract

### Subconscious Expects (–≤—ñ–¥ Clerk):

```python
state = {
    "message_id": str,        # Required
    "recorded": bool,         # Required (must be True)
    "message_content": str,   # Optional (can re-fetch)
    "message_role": str,      # Optional
}
```

### Subconscious Provides (–¥–ª—è Orchestrator):

```python
state = {
    # ... existing from Clerk ...
    
    # NEW from Subconscious
    "analyzed": bool,
    "context": {
        "recent_messages": [...],
        "similar_chunks": [...],
        "mentioned_entities": [...],
        "topics": [...],
        "conversation_continuity": float,
        # ... full ContextAnalysis
    },
    "error": str | None,
}
```

---

## Migration from Phase 1

### Existing Messages

–ö–æ–ª–∏ –∑–∞–ø—É—Å—Ç–∏–º–æ Phase 2, –≤ –ë–î –≤–∂–µ –±—É–¥—É—Ç—å messages –∑ Phase 1 (–±–µ–∑ chunks/entities).

**Strategy:**

1. **Lazy analysis:** –ê–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Ç—ñ–ª—å–∫–∏ –Ω–æ–≤—ñ messages
2. **Background job (–æ–ø—Ü—ñ–æ–Ω–∞–ª—å–Ω–æ):** –ü—Ä–æ–∞–Ω–∞–ª—ñ–∑—É–≤–∞—Ç–∏ —Å—Ç–∞—Ä—ñ messages –≤ —Ñ–æ–Ω—ñ
3. **Query fallback:** –Ø–∫—â–æ –Ω–µ–º–∞—î chunks –¥–ª—è —Å—Ç–∞—Ä–æ–≥–æ message, –≤–∏–∫–æ—Ä–∏—Å—Ç–∞—Ç–∏ raw content

```python
async def get_context_for_message(message_id: str):
    """Get context –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ legacy messages."""
    
    # Try to get processed chunks
    chunks = await repo.get_chunks_for_message(message_id)
    
    if chunks:
        # Phase 2 message - use chunks
        return await build_context_from_chunks(chunks)
    else:
        # Phase 1 message - fallback to raw content
        message = await repo.get_message(message_id)
        return await build_basic_context(message.content)
```

---

## Future Enhancements (Phase 2.5+)

### 1. Entity Resolution & Merging

```cypher
// Merge duplicate entities
MATCH (e1:Entity {canonical_name: "docker"}),
      (e2:Entity {canonical_name: "docker"})
WHERE e1.id < e2.id
// Merge relationships and properties
// Delete e2
```

### 2. Topic Clustering

```python
# K-means clustering –Ω–∞ entity embeddings
from sklearn.cluster import KMeans

entity_vectors = [e.embedding for e in entities]
kmeans = KMeans(n_clusters=5)
clusters = kmeans.fit_predict(entity_vectors)

# Clusters = topics
```

### 3. Temporal Decay

```python
# –°—Ç–∞—Ä—ñ chunks –º–µ–Ω—à —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ñ
def apply_temporal_decay(similarity: float, age_days: int) -> float:
    """–ó–Ω–∏–∑–∏—Ç–∏ similarity –¥–ª—è —Å—Ç–∞—Ä–∏—Ö chunks."""
    decay_factor = 0.95 ** (age_days / 30)  # -5% –∫–æ–∂–Ω—ñ 30 –¥–Ω—ñ–≤
    return similarity * decay_factor
```

### 4. Cross-User Knowledge (—è–∫—â–æ –±–∞–≥–∞—Ç–æ users)

```cypher
// Entities shared across users
MATCH (e:Entity)<-[:MENTIONS]-(m:Message)
WITH e, count(DISTINCT m.user_id) as user_count
WHERE user_count > 10
RETURN e.name, e.type, user_count
ORDER BY user_count DESC
```

---

## Quick Reference Checklist

**When implementing Subconscious:**
- [ ] Use semantic chunking (RecursiveCharacterTextSplitter)
- [ ] Batch embeddings (OpenAI API efficient usage)
- [ ] Extract entities —á–µ—Ä–µ–∑ structured output (JSON mode)
- [ ] Store temporal timestamps (valid_at, invalid_at)
- [ ] NO session filtering - temporal queries only
- [ ] Cosine similarity threshold = 0.7
- [ ] Graceful degradation –Ω–∞ –ø–æ–º–∏–ª–∫–∞—Ö
- [ ] Log with "üß† –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å:" prefix
- [ ] Return dict –∑ ChatState structure
- [ ] Orchestrator WAITS - synchronous pipeline

**When debugging Subconscious:**
- [ ] Check OpenAI API keys configured
- [ ] Check FalkorDB vector storage strategy
- [ ] Verify chunk sizes reasonable (<1500 chars)
- [ ] Check similarity threshold not too high
- [ ] Verify temporal queries work (no session filter)
- [ ] Check logs for "üß† –ü—ñ–¥—Å–≤—ñ–¥–æ–º—ñ—Å—Ç—å"
- [ ] Inspect state["context"] structure
- [ ] Monitor processing time (<1s expected)

---

## Comparison: Sessions vs Temporal

| Aspect | Session-Based (Wrong) | Temporal (Correct) |
|--------|----------------------|-------------------|
| **Memory scope** | Limited to session | –ë–µ–∑–º–µ–∂–Ω–∞ |
| **–©–æ –ø–∞–º'—è—Ç–∞—î–º–æ** | –¢—ñ–ª—å–∫–∏ current session | –í–µ—Å—å —á–∞—Å |
| **Query example** | `WHERE session_id = $id` | `WHERE timestamp < $time` |
| **–ü–µ—Ä–µ–≤–∞–≥–∏** | –ü—Ä–æ—Å—Ç—ñ—à–µ | –†–µ–∞–ª—ñ—Å—Ç–∏—á–Ω—ñ—à–µ |
| **User experience** | "–ü–æ—á–Ω–∏ –Ω–æ–≤—É —Å–µ—Å—ñ—é" | "–ü–∞–º'—è—Ç–∞—î—à 3 –º—ñ—Å—è—Ü—ñ —Ç–æ–º—É?.." |
| **–Ø–∫ –≤ Graphiti** | ‚ùå –ù—ñ | ‚úÖ –¢–∞–∫ |

---

**Version:** 2.0.0  
**Created:** November 10, 2025  
**Status:** –ê—Ä—Ö—ñ—Ç–µ–∫—Ç—É—Ä–∞ –∑–∞–≤–µ—Ä—à–µ–Ω–∞, –≥–æ—Ç–æ–≤–∞ –¥–æ —ñ–º–ø–ª–µ–º–µ–Ω—Ç–∞—Ü—ñ—ó  
**Dependencies:** OpenAI API, FalkorDB, Phase 1 (Clerk)  
**Next:** Phase 3 (Orchestrator Agent)
